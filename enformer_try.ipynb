{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddabdb38",
   "metadata": {},
   "source": [
    "# Playing around with Enformer and enformer_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f269a20",
   "metadata": {},
   "source": [
    "## We can run VSCode in the farm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ce8cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lustre/scratch126/gengen/teams/parts/sb79/enformer_loader_sgb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824f3e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node-13-17\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "node_name = socket.gethostname()\n",
    "print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "053ec518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores Available: 256\n"
     ]
    }
   ],
   "source": [
    "import multiprocess\n",
    "cores = multiprocess.cpu_count()\n",
    "print(\"Cores Available: \" + str(cores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfef7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test R code to check connection to a node\n",
    "print(\"Hello, R is running successfully!\")\n",
    "\n",
    "# Check the R version\n",
    "R.version.string\n",
    "\n",
    "# Check the current working directory\n",
    "getwd()\n",
    "# Generate a simple plot\n",
    "x <- rnorm(100)\n",
    "y <- rnorm(100)\n",
    "plot(x, y, main=\"Scatter plot of random points\", xlab=\"X-axis\", ylab=\"Y-axis\")\n",
    "quartz()  # Opens a new window for the plot (on Linux or macOS)\n",
    "plot(1:10, 1:10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a4e8f",
   "metadata": {},
   "source": [
    "R code is not that well suited to VSCode... but should be able to write scripts here anyway :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd39571",
   "metadata": {},
   "source": [
    "## Let's inspect the enformer_loader output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603817f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/hgi/installs/conda-audited/miniforge/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf336403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence import\n",
    "import pysam\n",
    "import pyfaidx\n",
    "\n",
    "# import fasta file and create function for retrieving sequences\n",
    "fa_loc = '/lustre/scratch126/gengen/projects/graft/Dataset/reference/hg38_galGal6_full/fasta/GRCh38.GRCg6a.full.renamed.merged.fa'\n",
    "pyfaidx.Faidx(fa_loc)\n",
    "fasta_open = pysam.Fastafile(fa_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9963d68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chromosome hg38_1 has length 248956422\n",
      "Chromosome hg38_10 has length 133797422\n",
      "Chromosome hg38_11 has length 135086622\n",
      "Chromosome hg38_12 has length 133275309\n",
      "Chromosome hg38_13 has length 114364328\n",
      "Chromosome hg38_14 has length 107043718\n",
      "Chromosome hg38_15 has length 101991189\n",
      "Chromosome hg38_16 has length 90338345\n",
      "Chromosome hg38_17 has length 83257441\n",
      "Chromosome hg38_18 has length 80373285\n",
      "Chromosome hg38_19 has length 58617616\n",
      "Chromosome hg38_2 has length 242193529\n",
      "Chromosome hg38_20 has length 64444167\n",
      "Chromosome hg38_21 has length 46709983\n",
      "Chromosome hg38_22 has length 50818468\n",
      "Chromosome hg38_3 has length 198295559\n",
      "Chromosome hg38_4 has length 190214555\n",
      "Chromosome hg38_5 has length 181538259\n",
      "Chromosome hg38_6 has length 170805979\n",
      "Chromosome hg38_7 has length 159345973\n",
      "Chromosome hg38_8 has length 145138636\n",
      "Chromosome hg38_9 has length 138394717\n",
      "Chromosome hg38_MT has length 16569\n",
      "Chromosome hg38_X has length 156040895\n",
      "Chromosome hg38_Y has length 57227415\n",
      "Chromosome hg38_KI270728.1 has length 1872759\n",
      "Chromosome hg38_KI270727.1 has length 448248\n",
      "Chromosome hg38_KI270442.1 has length 392061\n",
      "Chromosome hg38_KI270729.1 has length 280839\n",
      "Chromosome hg38_GL000225.1 has length 211173\n",
      "Chromosome hg38_KI270743.1 has length 210658\n",
      "Chromosome hg38_GL000008.2 has length 209709\n",
      "Chromosome hg38_GL000009.2 has length 201709\n",
      "Chromosome hg38_KI270747.1 has length 198735\n",
      "Chromosome hg38_KI270722.1 has length 194050\n",
      "Chromosome hg38_GL000194.1 has length 191469\n",
      "Chromosome hg38_KI270742.1 has length 186739\n",
      "Chromosome hg38_GL000205.2 has length 185591\n",
      "Chromosome hg38_GL000195.1 has length 182896\n",
      "Chromosome hg38_KI270736.1 has length 181920\n",
      "Chromosome hg38_KI270733.1 has length 179772\n",
      "Chromosome hg38_GL000224.1 has length 179693\n",
      "Chromosome hg38_GL000219.1 has length 179198\n",
      "Chromosome hg38_KI270719.1 has length 176845\n",
      "Chromosome hg38_GL000216.2 has length 176608\n",
      "Chromosome hg38_KI270712.1 has length 176043\n",
      "Chromosome hg38_KI270706.1 has length 175055\n",
      "Chromosome hg38_KI270725.1 has length 172810\n",
      "Chromosome hg38_KI270744.1 has length 168472\n",
      "Chromosome hg38_KI270734.1 has length 165050\n",
      "Chromosome hg38_GL000213.1 has length 164239\n",
      "Chromosome hg38_GL000220.1 has length 161802\n",
      "Chromosome hg38_KI270715.1 has length 161471\n",
      "Chromosome hg38_GL000218.1 has length 161147\n",
      "Chromosome hg38_KI270749.1 has length 158759\n",
      "Chromosome hg38_KI270741.1 has length 157432\n",
      "Chromosome hg38_GL000221.1 has length 155397\n",
      "Chromosome hg38_KI270716.1 has length 153799\n",
      "Chromosome hg38_KI270731.1 has length 150754\n",
      "Chromosome hg38_KI270751.1 has length 150742\n",
      "Chromosome hg38_KI270750.1 has length 148850\n",
      "Chromosome hg38_KI270519.1 has length 138126\n",
      "Chromosome hg38_GL000214.1 has length 137718\n",
      "Chromosome hg38_KI270708.1 has length 127682\n",
      "Chromosome hg38_KI270730.1 has length 112551\n",
      "Chromosome hg38_KI270438.1 has length 112505\n",
      "Chromosome hg38_KI270737.1 has length 103838\n",
      "Chromosome hg38_KI270721.1 has length 100316\n",
      "Chromosome hg38_KI270738.1 has length 99375\n",
      "Chromosome hg38_KI270748.1 has length 93321\n",
      "Chromosome hg38_KI270435.1 has length 92983\n",
      "Chromosome hg38_GL000208.1 has length 92689\n",
      "Chromosome hg38_KI270538.1 has length 91309\n",
      "Chromosome hg38_KI270756.1 has length 79590\n",
      "Chromosome hg38_KI270739.1 has length 73985\n",
      "Chromosome hg38_KI270757.1 has length 71251\n",
      "Chromosome hg38_KI270709.1 has length 66860\n",
      "Chromosome hg38_KI270746.1 has length 66486\n",
      "Chromosome hg38_KI270753.1 has length 62944\n",
      "Chromosome hg38_KI270589.1 has length 44474\n",
      "Chromosome hg38_KI270726.1 has length 43739\n",
      "Chromosome hg38_KI270735.1 has length 42811\n",
      "Chromosome hg38_KI270711.1 has length 42210\n",
      "Chromosome hg38_KI270745.1 has length 41891\n",
      "Chromosome hg38_KI270714.1 has length 41717\n",
      "Chromosome hg38_KI270732.1 has length 41543\n",
      "Chromosome hg38_KI270713.1 has length 40745\n",
      "Chromosome hg38_KI270754.1 has length 40191\n",
      "Chromosome hg38_KI270710.1 has length 40176\n",
      "Chromosome hg38_KI270717.1 has length 40062\n",
      "Chromosome hg38_KI270724.1 has length 39555\n",
      "Chromosome hg38_KI270720.1 has length 39050\n",
      "Chromosome hg38_KI270723.1 has length 38115\n",
      "Chromosome hg38_KI270718.1 has length 38054\n",
      "Chromosome hg38_KI270317.1 has length 37690\n",
      "Chromosome hg38_KI270740.1 has length 37240\n",
      "Chromosome hg38_KI270755.1 has length 36723\n",
      "Chromosome hg38_KI270707.1 has length 32032\n",
      "Chromosome hg38_KI270579.1 has length 31033\n",
      "Chromosome hg38_KI270752.1 has length 27745\n",
      "Chromosome hg38_KI270512.1 has length 22689\n",
      "Chromosome hg38_KI270322.1 has length 21476\n",
      "Chromosome hg38_GL000226.1 has length 15008\n",
      "Chromosome hg38_KI270311.1 has length 12399\n",
      "Chromosome hg38_KI270366.1 has length 8320\n",
      "Chromosome hg38_KI270511.1 has length 8127\n",
      "Chromosome hg38_KI270448.1 has length 7992\n",
      "Chromosome hg38_KI270521.1 has length 7642\n",
      "Chromosome hg38_KI270581.1 has length 7046\n",
      "Chromosome hg38_KI270582.1 has length 6504\n",
      "Chromosome hg38_KI270515.1 has length 6361\n",
      "Chromosome hg38_KI270588.1 has length 6158\n",
      "Chromosome hg38_KI270591.1 has length 5796\n",
      "Chromosome hg38_KI270522.1 has length 5674\n",
      "Chromosome hg38_KI270507.1 has length 5353\n",
      "Chromosome hg38_KI270590.1 has length 4685\n",
      "Chromosome hg38_KI270584.1 has length 4513\n",
      "Chromosome hg38_KI270320.1 has length 4416\n",
      "Chromosome hg38_KI270382.1 has length 4215\n",
      "Chromosome hg38_KI270468.1 has length 4055\n",
      "Chromosome hg38_KI270467.1 has length 3920\n",
      "Chromosome hg38_KI270362.1 has length 3530\n",
      "Chromosome hg38_KI270517.1 has length 3253\n",
      "Chromosome hg38_KI270593.1 has length 3041\n",
      "Chromosome hg38_KI270528.1 has length 2983\n",
      "Chromosome hg38_KI270587.1 has length 2969\n",
      "Chromosome hg38_KI270364.1 has length 2855\n",
      "Chromosome hg38_KI270371.1 has length 2805\n",
      "Chromosome hg38_KI270333.1 has length 2699\n",
      "Chromosome hg38_KI270374.1 has length 2656\n",
      "Chromosome hg38_KI270411.1 has length 2646\n",
      "Chromosome hg38_KI270414.1 has length 2489\n",
      "Chromosome hg38_KI270510.1 has length 2415\n",
      "Chromosome hg38_KI270390.1 has length 2387\n",
      "Chromosome hg38_KI270375.1 has length 2378\n",
      "Chromosome hg38_KI270420.1 has length 2321\n",
      "Chromosome hg38_KI270509.1 has length 2318\n",
      "Chromosome hg38_KI270315.1 has length 2276\n",
      "Chromosome hg38_KI270302.1 has length 2274\n",
      "Chromosome hg38_KI270518.1 has length 2186\n",
      "Chromosome hg38_KI270530.1 has length 2168\n",
      "Chromosome hg38_KI270304.1 has length 2165\n",
      "Chromosome hg38_KI270418.1 has length 2145\n",
      "Chromosome hg38_KI270424.1 has length 2140\n",
      "Chromosome hg38_KI270417.1 has length 2043\n",
      "Chromosome hg38_KI270508.1 has length 1951\n",
      "Chromosome hg38_KI270303.1 has length 1942\n",
      "Chromosome hg38_KI270381.1 has length 1930\n",
      "Chromosome hg38_KI270529.1 has length 1899\n",
      "Chromosome hg38_KI270425.1 has length 1884\n",
      "Chromosome hg38_KI270396.1 has length 1880\n",
      "Chromosome hg38_KI270363.1 has length 1803\n",
      "Chromosome hg38_KI270386.1 has length 1788\n",
      "Chromosome hg38_KI270465.1 has length 1774\n",
      "Chromosome hg38_KI270383.1 has length 1750\n",
      "Chromosome hg38_KI270384.1 has length 1658\n",
      "Chromosome hg38_KI270330.1 has length 1652\n",
      "Chromosome hg38_KI270372.1 has length 1650\n",
      "Chromosome hg38_KI270548.1 has length 1599\n",
      "Chromosome hg38_KI270580.1 has length 1553\n",
      "Chromosome hg38_KI270387.1 has length 1537\n",
      "Chromosome hg38_KI270391.1 has length 1484\n",
      "Chromosome hg38_KI270305.1 has length 1472\n",
      "Chromosome hg38_KI270373.1 has length 1451\n",
      "Chromosome hg38_KI270422.1 has length 1445\n",
      "Chromosome hg38_KI270316.1 has length 1444\n",
      "Chromosome hg38_KI270340.1 has length 1428\n",
      "Chromosome hg38_KI270338.1 has length 1428\n",
      "Chromosome hg38_KI270583.1 has length 1400\n",
      "Chromosome hg38_KI270334.1 has length 1368\n",
      "Chromosome hg38_KI270429.1 has length 1361\n",
      "Chromosome hg38_KI270393.1 has length 1308\n",
      "Chromosome hg38_KI270516.1 has length 1300\n",
      "Chromosome hg38_KI270389.1 has length 1298\n",
      "Chromosome hg38_KI270466.1 has length 1233\n",
      "Chromosome hg38_KI270388.1 has length 1216\n",
      "Chromosome hg38_KI270544.1 has length 1202\n",
      "Chromosome hg38_KI270310.1 has length 1201\n",
      "Chromosome hg38_KI270412.1 has length 1179\n",
      "Chromosome hg38_KI270395.1 has length 1143\n",
      "Chromosome hg38_KI270376.1 has length 1136\n",
      "Chromosome hg38_KI270337.1 has length 1121\n",
      "Chromosome hg38_KI270335.1 has length 1048\n",
      "Chromosome hg38_KI270378.1 has length 1048\n",
      "Chromosome hg38_KI270379.1 has length 1045\n",
      "Chromosome hg38_KI270329.1 has length 1040\n",
      "Chromosome hg38_KI270419.1 has length 1029\n",
      "Chromosome hg38_KI270336.1 has length 1026\n",
      "Chromosome hg38_KI270312.1 has length 998\n",
      "Chromosome hg38_KI270539.1 has length 993\n",
      "Chromosome hg38_KI270385.1 has length 990\n",
      "Chromosome hg38_KI270423.1 has length 981\n",
      "Chromosome hg38_KI270392.1 has length 971\n",
      "Chromosome hg38_KI270394.1 has length 970\n",
      "Chromosome gg6_MT has length 16775\n",
      "Chromosome gg6_W has length 6813114\n",
      "Chromosome gg6_Z has length 82529921\n",
      "Chromosome gg6_1 has length 197608386\n",
      "Chromosome gg6_2 has length 149682049\n",
      "Chromosome gg6_3 has length 110838418\n",
      "Chromosome gg6_4 has length 91315245\n",
      "Chromosome gg6_5 has length 59809098\n",
      "Chromosome gg6_6 has length 36374701\n",
      "Chromosome gg6_7 has length 36742308\n",
      "Chromosome gg6_8 has length 30219446\n",
      "Chromosome gg6_9 has length 24153086\n",
      "Chromosome gg6_10 has length 21119840\n",
      "Chromosome gg6_11 has length 20200042\n",
      "Chromosome gg6_12 has length 20387278\n",
      "Chromosome gg6_13 has length 19166714\n",
      "Chromosome gg6_14 has length 16219308\n",
      "Chromosome gg6_15 has length 13062184\n",
      "Chromosome gg6_16 has length 2844601\n",
      "Chromosome gg6_17 has length 10762512\n",
      "Chromosome gg6_18 has length 11373140\n",
      "Chromosome gg6_19 has length 10323212\n",
      "Chromosome gg6_20 has length 13897287\n",
      "Chromosome gg6_21 has length 6844979\n",
      "Chromosome gg6_22 has length 5459462\n",
      "Chromosome gg6_23 has length 6149580\n",
      "Chromosome gg6_24 has length 6491222\n",
      "Chromosome gg6_25 has length 3980610\n",
      "Chromosome gg6_26 has length 6055710\n",
      "Chromosome gg6_27 has length 8080432\n",
      "Chromosome gg6_28 has length 5116882\n",
      "Chromosome gg6_30 has length 1818525\n",
      "Chromosome gg6_31 has length 6153034\n",
      "Chromosome gg6_32 has length 725831\n",
      "Chromosome gg6_33 has length 7821666\n",
      "Chromosome gg6_KZ626839.1 has length 2023903\n",
      "Chromosome gg6_KZ626837.1 has length 2003471\n",
      "Chromosome gg6_KZ626838.1 has length 1977309\n",
      "Chromosome gg6_KZ626835.1 has length 1222195\n",
      "Chromosome gg6_KZ626836.1 has length 900507\n",
      "Chromosome gg6_KZ626833.1 has length 696307\n",
      "Chromosome gg6_KZ626834.1 has length 665899\n",
      "Chromosome gg6_KZ626831.1 has length 468536\n",
      "Chromosome gg6_KZ626832.1 has length 199634\n",
      "Chromosome gg6_KZ626829.1 has length 177577\n",
      "Chromosome gg6_KZ626819.1 has length 149503\n",
      "Chromosome gg6_KZ626830.1 has length 94788\n",
      "Chromosome gg6_KZ626827.1 has length 61939\n",
      "Chromosome gg6_KZ626828.1 has length 53716\n",
      "Chromosome gg6_AADN05001291.1 has length 52554\n",
      "Chromosome gg6_KZ626826.1 has length 48347\n",
      "Chromosome gg6_AADN05001235.1 has length 42067\n",
      "Chromosome gg6_AADN05001385.1 has length 39545\n",
      "Chromosome gg6_AADN05001349.1 has length 34471\n",
      "Chromosome gg6_AADN05001188.1 has length 34291\n",
      "Chromosome gg6_AADN05001187.1 has length 33802\n",
      "Chromosome gg6_KZ626824.1 has length 33731\n",
      "Chromosome gg6_AADN05001513.1 has length 33531\n",
      "Chromosome gg6_AADN05001353.1 has length 33083\n",
      "Chromosome gg6_AADN05000202.1 has length 33030\n",
      "Chromosome gg6_AADN05001371.1 has length 30818\n",
      "Chromosome gg6_AADN05001420.1 has length 30439\n",
      "Chromosome gg6_AADN05001193.1 has length 30225\n",
      "Chromosome gg6_AADN05001498.1 has length 30157\n",
      "Chromosome gg6_AADN05001414.1 has length 29697\n",
      "Chromosome gg6_AADN05000531.1 has length 28243\n",
      "Chromosome gg6_AADN05001370.1 has length 28212\n",
      "Chromosome gg6_AADN05000525.1 has length 27963\n",
      "Chromosome gg6_AADN05001329.1 has length 27830\n",
      "Chromosome gg6_AADN05001466.1 has length 27808\n",
      "Chromosome gg6_AADN05001240.1 has length 27130\n",
      "Chromosome gg6_AADN05001306.1 has length 27104\n",
      "Chromosome gg6_AADN05001220.1 has length 26960\n",
      "Chromosome gg6_AADN05001219.1 has length 26595\n",
      "Chromosome gg6_AADN05001361.1 has length 26310\n",
      "Chromosome gg6_AADN05001399.1 has length 26298\n",
      "Chromosome gg6_AADN05001393.1 has length 26150\n",
      "Chromosome gg6_AADN05001400.1 has length 26131\n",
      "Chromosome gg6_AADN05001292.1 has length 25700\n",
      "Chromosome gg6_AADN05001357.1 has length 25466\n",
      "Chromosome gg6_AADN05001476.1 has length 25409\n",
      "Chromosome gg6_AADN05001287.1 has length 25301\n",
      "Chromosome gg6_AADN05001464.1 has length 24712\n",
      "Chromosome gg6_AADN05001432.1 has length 24315\n",
      "Chromosome gg6_AADN05001241.1 has length 24179\n",
      "Chromosome gg6_AADN05001518.1 has length 24105\n",
      "Chromosome gg6_AADN05001504.1 has length 23907\n",
      "Chromosome gg6_AADN05001221.1 has length 23766\n",
      "Chromosome gg6_AADN05001307.1 has length 23748\n",
      "Chromosome gg6_AADN05001423.1 has length 23209\n",
      "Chromosome gg6_AADN05001288.1 has length 23190\n",
      "Chromosome gg6_AADN05001326.1 has length 22982\n",
      "Chromosome gg6_AADN05001424.1 has length 22964\n",
      "Chromosome gg6_AADN05001460.1 has length 22641\n",
      "Chromosome gg6_AADN05001222.1 has length 22621\n",
      "Chromosome gg6_AADN05001405.1 has length 22381\n",
      "Chromosome gg6_AADN05001505.1 has length 22379\n",
      "Chromosome gg6_AADN05001390.1 has length 22289\n",
      "Chromosome gg6_AADN05001172.1 has length 21751\n",
      "Chromosome gg6_AADN05001330.1 has length 21699\n",
      "Chromosome gg6_AADN05001350.1 has length 21677\n",
      "Chromosome gg6_AADN05001223.1 has length 21586\n",
      "Chromosome gg6_AADN05001224.1 has length 21580\n",
      "Chromosome gg6_AADN05001290.1 has length 21511\n",
      "Chromosome gg6_AADN05001343.1 has length 21427\n",
      "Chromosome gg6_AADN05001355.1 has length 21266\n",
      "Chromosome gg6_AADN05001145.1 has length 20717\n",
      "Chromosome gg6_AADN05001227.1 has length 20496\n",
      "Chromosome gg6_AADN05001250.1 has length 20494\n",
      "Chromosome gg6_AADN05001483.1 has length 20270\n",
      "Chromosome gg6_AADN05001467.1 has length 19928\n",
      "Chromosome gg6_AADN05001293.1 has length 19903\n",
      "Chromosome gg6_AADN05001433.1 has length 19850\n",
      "Chromosome gg6_AADN05001425.1 has length 19721\n",
      "Chromosome gg6_AADN05001275.1 has length 19536\n",
      "Chromosome gg6_AADN05001461.1 has length 19471\n",
      "Chromosome gg6_AADN05001434.1 has length 19204\n",
      "Chromosome gg6_AADN05001422.1 has length 19095\n",
      "Chromosome gg6_AADN05001482.1 has length 19032\n",
      "Chromosome gg6_AADN05001197.1 has length 18989\n",
      "Chromosome gg6_AADN05001160.1 has length 18922\n",
      "Chromosome gg6_AADN05001217.1 has length 18845\n",
      "Chromosome gg6_AADN05001202.1 has length 18705\n",
      "Chromosome gg6_AADN05001366.1 has length 18507\n",
      "Chromosome gg6_AADN05001503.1 has length 18412\n",
      "Chromosome gg6_AADN05001496.1 has length 18344\n",
      "Chromosome gg6_AADN05001416.1 has length 18101\n",
      "Chromosome gg6_AADN05001323.1 has length 18034\n",
      "Chromosome gg6_AADN05001435.1 has length 18028\n",
      "Chromosome gg6_AADN05001412.1 has length 17442\n",
      "Chromosome gg6_AADN05001478.1 has length 17402\n",
      "Chromosome gg6_AADN05001468.1 has length 17321\n",
      "Chromosome gg6_AADN05001308.1 has length 17237\n",
      "Chromosome gg6_AADN05001237.1 has length 17224\n",
      "Chromosome gg6_AADN05001506.1 has length 17217\n",
      "Chromosome gg6_AADN05001377.1 has length 17007\n",
      "Chromosome gg6_AADN05001264.1 has length 16505\n",
      "Chromosome gg6_AADN05001225.1 has length 16464\n",
      "Chromosome gg6_AADN05001436.1 has length 16320\n",
      "Chromosome gg6_AADN05001331.1 has length 16245\n",
      "Chromosome gg6_AADN05001348.1 has length 16233\n",
      "Chromosome gg6_AADN05001267.1 has length 16157\n",
      "Chromosome gg6_AADN05001497.1 has length 16140\n",
      "Chromosome gg6_AADN05001437.1 has length 16129\n",
      "Chromosome gg6_AADN05001372.1 has length 16106\n",
      "Chromosome gg6_AADN05001469.1 has length 16043\n",
      "Chromosome gg6_AADN05001378.1 has length 16031\n",
      "Chromosome gg6_AADN05001199.1 has length 15929\n",
      "Chromosome gg6_AADN05001289.1 has length 15680\n",
      "Chromosome gg6_AADN05001345.1 has length 15663\n",
      "Chromosome gg6_AADN05001312.1 has length 15638\n",
      "Chromosome gg6_AADN05001439.1 has length 15587\n",
      "Chromosome gg6_AADN05001214.1 has length 15492\n",
      "Chromosome gg6_AADN05001438.1 has length 15438\n",
      "Chromosome gg6_AADN05001233.1 has length 15417\n",
      "Chromosome gg6_AADN05001205.1 has length 15387\n",
      "Chromosome gg6_AADN05001226.1 has length 15167\n",
      "Chromosome gg6_AADN05001234.1 has length 15134\n",
      "Chromosome gg6_AADN05001271.1 has length 15132\n",
      "Chromosome gg6_AADN05001283.1 has length 15049\n",
      "Chromosome gg6_AADN05001389.1 has length 15003\n",
      "Chromosome gg6_AADN05001462.1 has length 14810\n",
      "Chromosome gg6_AADN05001191.1 has length 14773\n",
      "Chromosome gg6_AADN05001524.1 has length 14715\n",
      "Chromosome gg6_AADN05001302.1 has length 14655\n",
      "Chromosome gg6_AADN05001441.1 has length 14613\n",
      "Chromosome gg6_AADN05001440.1 has length 14604\n",
      "Chromosome gg6_AADN05001165.1 has length 14494\n",
      "Chromosome gg6_AADN05001459.1 has length 14330\n",
      "Chromosome gg6_AADN05001386.1 has length 14298\n",
      "Chromosome gg6_AADN05001319.1 has length 14205\n",
      "Chromosome gg6_AADN05001170.1 has length 14131\n",
      "Chromosome gg6_AADN05001167.1 has length 14086\n",
      "Chromosome gg6_AADN05001185.1 has length 14047\n",
      "Chromosome gg6_AADN05001477.1 has length 13985\n",
      "Chromosome gg6_AADN05001512.1 has length 13913\n",
      "Chromosome gg6_AADN05001410.1 has length 13829\n",
      "Chromosome gg6_AADN05001311.1 has length 13798\n",
      "Chromosome gg6_AADN05001402.1 has length 13793\n",
      "Chromosome gg6_AADN05001480.1 has length 13691\n",
      "Chromosome gg6_AADN05001255.1 has length 13671\n",
      "Chromosome gg6_AADN05001203.1 has length 13599\n",
      "Chromosome gg6_AADN05001195.1 has length 13513\n",
      "Chromosome gg6_AADN05001201.1 has length 13240\n",
      "Chromosome gg6_AADN05001232.1 has length 13224\n",
      "Chromosome gg6_AADN05001309.1 has length 13157\n",
      "Chromosome gg6_AADN05001327.1 has length 13139\n",
      "Chromosome gg6_AADN05001324.1 has length 13021\n",
      "Chromosome gg6_AADN05001337.1 has length 12987\n",
      "Chromosome gg6_AADN05001417.1 has length 12986\n",
      "Chromosome gg6_AADN05001356.1 has length 12789\n",
      "Chromosome gg6_AADN05001431.1 has length 12728\n",
      "Chromosome gg6_AADN05001200.1 has length 12721\n",
      "Chromosome gg6_AADN05001256.1 has length 12562\n",
      "Chromosome gg6_AADN05001146.1 has length 12537\n",
      "Chromosome gg6_AADN05001455.1 has length 12430\n",
      "Chromosome gg6_AADN05001421.1 has length 12389\n",
      "Chromosome gg6_AADN05001365.1 has length 12152\n",
      "Chromosome gg6_AADN05001388.1 has length 12129\n",
      "Chromosome gg6_AADN05001442.1 has length 12127\n",
      "Chromosome gg6_AADN05001320.1 has length 12125\n",
      "Chromosome gg6_AADN05001415.1 has length 12040\n",
      "Chromosome gg6_AADN05001367.1 has length 11985\n",
      "Chromosome gg6_AADN05001375.1 has length 11970\n",
      "Chromosome gg6_AADN05001245.1 has length 11891\n",
      "Chromosome gg6_AADN05001313.1 has length 11743\n",
      "Chromosome gg6_AADN05001276.1 has length 11663\n",
      "Chromosome gg6_AADN05001407.1 has length 11473\n",
      "Chromosome gg6_AADN05001489.1 has length 11376\n",
      "Chromosome gg6_AADN05001362.1 has length 11340\n",
      "Chromosome gg6_AADN05001406.1 has length 11305\n",
      "Chromosome gg6_AADN05001521.1 has length 11254\n",
      "Chromosome gg6_AADN05001443.1 has length 11212\n",
      "Chromosome gg6_AADN05001369.1 has length 11204\n",
      "Chromosome gg6_AADN05001238.1 has length 11068\n",
      "Chromosome gg6_AADN05001444.1 has length 11066\n",
      "Chromosome gg6_AADN05001322.1 has length 10993\n",
      "Chromosome gg6_AADN05001376.1 has length 10984\n",
      "Chromosome gg6_AADN05001229.1 has length 10928\n",
      "Chromosome gg6_AADN05001470.1 has length 10871\n",
      "Chromosome gg6_AADN05001373.1 has length 10869\n",
      "Chromosome gg6_AADN05001257.1 has length 10816\n",
      "Chromosome gg6_AADN05001228.1 has length 10724\n",
      "Chromosome gg6_AADN05001374.1 has length 10691\n",
      "Chromosome gg6_AADN05001452.1 has length 10551\n",
      "Chromosome gg6_AADN05001474.1 has length 10473\n",
      "Chromosome gg6_AADN05001446.1 has length 10371\n",
      "Chromosome gg6_AADN05001453.1 has length 10371\n",
      "Chromosome gg6_AADN05001265.1 has length 10224\n",
      "Chromosome gg6_AADN05001445.1 has length 10162\n",
      "Chromosome gg6_AADN05001499.1 has length 10106\n",
      "Chromosome gg6_AADN05001430.1 has length 10066\n",
      "Chromosome gg6_AADN05001368.1 has length 10056\n",
      "Chromosome gg6_AADN05001151.1 has length 10010\n",
      "Chromosome gg6_AADN05001447.1 has length 9935\n",
      "Chromosome gg6_AADN05001274.1 has length 9888\n",
      "Chromosome gg6_AADN05001179.1 has length 9858\n",
      "Chromosome gg6_AADN05001486.1 has length 9696\n",
      "Chromosome gg6_AADN05001519.1 has length 9688\n",
      "Chromosome gg6_AADN05001328.1 has length 9619\n",
      "Chromosome gg6_AADN05001448.1 has length 9617\n",
      "Chromosome gg6_AADN05001426.1 has length 9607\n",
      "Chromosome gg6_AADN05001484.1 has length 9579\n",
      "Chromosome gg6_AADN05001161.1 has length 9568\n",
      "Chromosome gg6_AADN05001204.1 has length 9481\n",
      "Chromosome gg6_AADN05001258.1 has length 9469\n",
      "Chromosome gg6_AADN05001171.1 has length 9308\n",
      "Chromosome gg6_AADN05001523.1 has length 9182\n",
      "Chromosome gg6_AADN05001314.1 has length 9172\n",
      "Chromosome gg6_AADN05001411.1 has length 9118\n",
      "Chromosome gg6_AADN05001192.1 has length 9034\n",
      "Chromosome gg6_AADN05001491.1 has length 9032\n",
      "Chromosome gg6_AADN05001454.1 has length 8999\n",
      "Chromosome gg6_AADN05001242.1 has length 8915\n",
      "Chromosome gg6_AADN05001251.1 has length 8913\n",
      "Chromosome gg6_AADN05001427.1 has length 8909\n",
      "Chromosome gg6_AADN05001508.1 has length 8891\n",
      "Chromosome gg6_AADN05001487.1 has length 8846\n",
      "Chromosome gg6_AADN05001418.1 has length 8788\n",
      "Chromosome gg6_AADN05001463.1 has length 8749\n",
      "Chromosome gg6_AADN05001198.1 has length 8722\n",
      "Chromosome gg6_AADN05001272.1 has length 8656\n",
      "Chromosome gg6_AADN05001278.1 has length 8637\n",
      "Chromosome gg6_AADN05001449.1 has length 8559\n",
      "Chromosome gg6_AADN05001465.1 has length 8516\n",
      "Chromosome gg6_AADN05001381.1 has length 8464\n",
      "Chromosome gg6_AADN05001516.1 has length 8294\n",
      "Chromosome gg6_AADN05001471.1 has length 8275\n",
      "Chromosome gg6_AADN05001457.1 has length 8267\n",
      "Chromosome gg6_AADN05001157.1 has length 8247\n",
      "Chromosome gg6_AADN05001182.1 has length 8242\n",
      "Chromosome gg6_AADN05001248.1 has length 8189\n",
      "Chromosome gg6_AADN05001351.1 has length 8180\n",
      "Chromosome gg6_AADN05001450.1 has length 8166\n",
      "Chromosome gg6_AADN05001341.1 has length 8152\n",
      "Chromosome gg6_AADN05001517.1 has length 8118\n",
      "Chromosome gg6_AADN05001254.1 has length 8110\n",
      "Chromosome gg6_AADN05001280.1 has length 8038\n",
      "Chromosome gg6_AADN05001339.1 has length 8025\n",
      "Chromosome gg6_AADN05001206.1 has length 7923\n",
      "Chromosome gg6_AADN05001259.1 has length 7918\n",
      "Chromosome gg6_AADN05001429.1 has length 7878\n",
      "Chromosome gg6_AADN05001451.1 has length 7765\n",
      "Chromosome gg6_AADN05001382.1 has length 7675\n",
      "Chromosome gg6_AADN05001315.1 has length 7655\n",
      "Chromosome gg6_AADN05001347.1 has length 7621\n",
      "Chromosome gg6_AADN05001525.1 has length 7511\n",
      "Chromosome gg6_AADN05000232.1 has length 7419\n",
      "Chromosome gg6_AADN05001510.1 has length 7336\n",
      "Chromosome gg6_AADN05001408.1 has length 7298\n",
      "Chromosome gg6_AADN05001230.1 has length 7246\n",
      "Chromosome gg6_AADN05001522.1 has length 7090\n",
      "Chromosome gg6_AADN05001501.1 has length 7074\n",
      "Chromosome gg6_AADN05001404.1 has length 6958\n",
      "Chromosome gg6_AADN05001413.1 has length 6929\n",
      "Chromosome gg6_AADN05001394.1 has length 6925\n",
      "Chromosome gg6_AADN05001216.1 has length 6885\n",
      "Chromosome gg6_AADN05001542.1 has length 6881\n",
      "Chromosome gg6_AADN05001428.1 has length 6830\n",
      "Chromosome gg6_AADN05001305.1 has length 6824\n",
      "Chromosome gg6_AADN05001173.1 has length 6824\n",
      "Chromosome gg6_AADN05001211.1 has length 6814\n",
      "Chromosome gg6_AADN05001401.1 has length 6808\n",
      "Chromosome gg6_AADN05001475.1 has length 6764\n",
      "Chromosome gg6_AADN05001409.1 has length 6698\n",
      "Chromosome gg6_AADN05001500.1 has length 6668\n",
      "Chromosome gg6_AADN05001342.1 has length 6638\n",
      "Chromosome gg6_AADN05001149.1 has length 6586\n",
      "Chromosome gg6_AADN05001249.1 has length 6564\n",
      "Chromosome gg6_AADN05001168.1 has length 6539\n",
      "Chromosome gg6_AADN05001488.1 has length 6414\n",
      "Chromosome gg6_AADN05001543.1 has length 6378\n",
      "Chromosome gg6_AADN05001189.1 has length 6349\n",
      "Chromosome gg6_AADN05001395.1 has length 6324\n",
      "Chromosome gg6_AADN05001419.1 has length 6318\n",
      "Chromosome gg6_AADN05001218.1 has length 6272\n",
      "Chromosome gg6_AADN05001243.1 has length 6269\n",
      "Chromosome gg6_AADN05001481.1 has length 6243\n",
      "Chromosome gg6_AADN05001456.1 has length 6185\n",
      "Chromosome gg6_AADN05001364.1 has length 6066\n",
      "Chromosome gg6_AADN05001284.1 has length 6042\n",
      "Chromosome gg6_AADN05001281.1 has length 5955\n",
      "Chromosome gg6_AADN05001175.1 has length 5861\n",
      "Chromosome gg6_AADN05001231.1 has length 5791\n",
      "Chromosome gg6_AADN05001338.1 has length 5694\n",
      "Chromosome gg6_AADN05001183.1 has length 5685\n",
      "Chromosome gg6_AADN05001363.1 has length 5662\n",
      "Chromosome gg6_AADN05001174.1 has length 5637\n",
      "Chromosome gg6_AADN05001458.1 has length 5587\n",
      "Chromosome gg6_AADN05001358.1 has length 5521\n",
      "Chromosome gg6_AADN05001520.1 has length 5419\n",
      "Chromosome gg6_AADN05001316.1 has length 5289\n",
      "Chromosome gg6_AADN05001212.1 has length 5230\n",
      "Chromosome gg6_AADN05001169.1 has length 5224\n",
      "Chromosome gg6_AADN05001494.1 has length 5199\n",
      "Chromosome gg6_AADN05001336.1 has length 5075\n",
      "Chromosome gg6_AADN05001359.1 has length 5044\n",
      "Chromosome gg6_AADN05001387.1 has length 4840\n",
      "Chromosome gg6_AADN05001354.1 has length 4808\n",
      "Chromosome gg6_AADN05001473.1 has length 4777\n",
      "Chromosome gg6_AADN05001398.1 has length 4766\n",
      "Chromosome gg6_AADN05001263.1 has length 4730\n",
      "Chromosome gg6_AADN05001239.1 has length 4685\n",
      "Chromosome gg6_AADN05001166.1 has length 4675\n",
      "Chromosome gg6_AADN05001493.1 has length 4652\n",
      "Chromosome gg6_AADN05001317.1 has length 4621\n",
      "Chromosome gg6_AADN05001247.1 has length 4617\n",
      "Chromosome gg6_AADN05001207.1 has length 4511\n",
      "Chromosome gg6_KZ626825.1 has length 4451\n",
      "Chromosome gg6_AADN05001479.1 has length 4448\n",
      "Chromosome gg6_AADN05001391.1 has length 4360\n",
      "Chromosome gg6_AADN05001492.1 has length 4359\n",
      "Chromosome gg6_AADN05001262.1 has length 4266\n",
      "Chromosome gg6_AADN05001260.1 has length 4245\n",
      "Chromosome gg6_AADN05001301.1 has length 4206\n",
      "Chromosome gg6_AADN05001156.1 has length 4198\n",
      "Chromosome gg6_AADN05001209.1 has length 4154\n",
      "Chromosome gg6_AADN05001277.1 has length 4087\n",
      "Chromosome gg6_AADN05001184.1 has length 4040\n",
      "Chromosome gg6_AADN05001396.1 has length 4037\n",
      "Chromosome gg6_AADN05001155.1 has length 3973\n",
      "Chromosome gg6_AADN05001296.1 has length 3863\n",
      "Chromosome gg6_AADN05001384.1 has length 3847\n",
      "Chromosome gg6_AADN05001294.1 has length 3687\n",
      "Chromosome gg6_AADN05001180.1 has length 3680\n",
      "Chromosome gg6_AADN05001321.1 has length 3660\n",
      "Chromosome gg6_AADN05001556.1 has length 3657\n",
      "Chromosome gg6_AADN05001194.1 has length 3647\n",
      "Chromosome gg6_AADN05001304.1 has length 3584\n",
      "Chromosome gg6_AADN05001208.1 has length 3581\n",
      "Chromosome gg6_AADN05001511.1 has length 3531\n",
      "Chromosome gg6_AADN05001268.1 has length 3516\n",
      "Chromosome gg6_AADN05001397.1 has length 3504\n",
      "Chromosome gg6_AADN05001236.1 has length 3436\n",
      "Chromosome gg6_AADN05001176.1 has length 3346\n",
      "Chromosome gg6_AADN05001298.1 has length 3094\n",
      "Chromosome gg6_AADN05001380.1 has length 3019\n",
      "Chromosome gg6_AADN05001164.1 has length 2991\n",
      "Chromosome gg6_AADN05001379.1 has length 2985\n",
      "Chromosome gg6_AADN05001252.1 has length 2979\n",
      "Chromosome gg6_AADN05001285.1 has length 2948\n",
      "Chromosome gg6_AADN05001360.1 has length 2928\n",
      "Chromosome gg6_AADN05001186.1 has length 2902\n",
      "Chromosome gg6_AADN05001266.1 has length 2898\n",
      "Chromosome gg6_AADN05001269.1 has length 2883\n",
      "Chromosome gg6_AADN05001261.1 has length 2758\n",
      "Chromosome gg6_AADN05001544.1 has length 2663\n",
      "Chromosome gg6_AADN05001392.1 has length 2615\n",
      "Chromosome gg6_AADN05001325.1 has length 2610\n",
      "Chromosome gg6_AADN05001563.1 has length 2599\n",
      "Chromosome gg6_AADN05001509.1 has length 2568\n",
      "Chromosome gg6_AADN05001150.1 has length 2566\n",
      "Chromosome gg6_AADN05001502.1 has length 2555\n",
      "Chromosome gg6_AADN05001310.1 has length 2520\n",
      "Chromosome gg6_AADN05001550.1 has length 2481\n",
      "Chromosome gg6_AADN05001318.1 has length 2462\n",
      "Chromosome gg6_AADN05001485.1 has length 2402\n",
      "Chromosome gg6_AADN05001403.1 has length 2384\n",
      "Chromosome gg6_AADN05001560.1 has length 2308\n",
      "Chromosome gg6_AADN05001215.1 has length 2262\n",
      "Chromosome gg6_AADN05001162.1 has length 2260\n",
      "Chromosome gg6_AADN05001178.1 has length 2214\n",
      "Chromosome gg6_AADN05001554.1 has length 2157\n",
      "Chromosome gg6_AADN05000623.1 has length 2149\n",
      "Chromosome gg6_AADN05001196.1 has length 2092\n",
      "Chromosome gg6_AADN05001333.1 has length 2087\n",
      "Chromosome gg6_AADN05001190.1 has length 2071\n",
      "Chromosome gg6_AADN05001270.1 has length 2034\n",
      "Chromosome gg6_AADN05001273.1 has length 2033\n",
      "Chromosome gg6_AADN05001154.1 has length 2003\n",
      "Chromosome gg6_AADN05001163.1 has length 1935\n",
      "Chromosome gg6_AADN05001279.1 has length 1908\n",
      "Chromosome gg6_AADN05001297.1 has length 1884\n",
      "Chromosome gg6_AADN05001295.1 has length 1840\n",
      "Chromosome gg6_AADN05001559.1 has length 1834\n",
      "Chromosome gg6_AADN05001540.1 has length 1800\n",
      "Chromosome gg6_AADN05001148.1 has length 1783\n",
      "Chromosome gg6_AADN05001383.1 has length 1782\n",
      "Chromosome gg6_AADN05001177.1 has length 1765\n",
      "Chromosome gg6_AADN05001352.1 has length 1756\n",
      "Chromosome gg6_AADN05001514.1 has length 1752\n",
      "Chromosome gg6_AADN05001299.1 has length 1748\n",
      "Chromosome gg6_AADN05001344.1 has length 1664\n",
      "Chromosome gg6_AADN05001507.1 has length 1662\n",
      "Chromosome gg6_AADN05001152.1 has length 1643\n",
      "Chromosome gg6_AADN05001495.1 has length 1589\n",
      "Chromosome gg6_AADN05001515.1 has length 1579\n",
      "Chromosome gg6_AADN05001472.1 has length 1568\n",
      "Chromosome gg6_AADN05001334.1 has length 1547\n",
      "Chromosome gg6_AADN05001153.1 has length 1519\n",
      "Chromosome gg6_AADN05001332.1 has length 1453\n",
      "Chromosome gg6_AADN05001300.1 has length 1394\n",
      "Chromosome gg6_AADN05001244.1 has length 1381\n",
      "Chromosome gg6_AADN05001253.1 has length 1367\n",
      "Chromosome gg6_AADN05001548.1 has length 1264\n",
      "Chromosome gg6_AADN05001246.1 has length 1173\n",
      "Chromosome gg6_AADN05001303.1 has length 1167\n",
      "Chromosome gg6_AADN05001539.1 has length 1131\n",
      "Chromosome gg6_AADN05001213.1 has length 1115\n",
      "Chromosome gg6_AADN05001490.1 has length 1086\n",
      "Chromosome gg6_AADN05001555.1 has length 966\n",
      "Chromosome gg6_AADN05001340.1 has length 955\n",
      "Chromosome gg6_AADN05001286.1 has length 950\n",
      "Chromosome gg6_AADN05001282.1 has length 918\n",
      "Chromosome gg6_AADN05001552.1 has length 848\n",
      "Chromosome gg6_AADN05001557.1 has length 806\n",
      "Chromosome gg6_AADN05001541.1 has length 791\n",
      "Chromosome gg6_AADN05001158.1 has length 765\n",
      "Chromosome gg6_AADN05001561.1 has length 678\n",
      "Chromosome gg6_AADN05001335.1 has length 677\n",
      "Chromosome gg6_AADN05001181.1 has length 676\n",
      "Chromosome gg6_AADN05001147.1 has length 644\n",
      "Chromosome gg6_AADN05001210.1 has length 623\n",
      "Chromosome gg6_AADN05001159.1 has length 597\n",
      "Chromosome gg6_AADN05001558.1 has length 579\n",
      "Chromosome gg6_AADN05001346.1 has length 555\n",
      "Chromosome gg6_AADN05001551.1 has length 509\n",
      "Chromosome gg6_AADN05001553.1 has length 345\n",
      "Chromosome gg6_AADN05001545.1 has length 342\n",
      "Chromosome gg6_AADN05001564.1 has length 322\n",
      "Chromosome gg6_AADN05001546.1 has length 246\n",
      "Chromosome gg6_AADN05001547.1 has length 195\n",
      "Chromosome gg6_AADN05001549.1 has length 182\n",
      "Chromosome gg6_AADN05001562.1 has length 87\n"
     ]
    }
   ],
   "source": [
    "chromlength = []\n",
    "chromname = []\n",
    "with pysam.Fastafile(fa_loc) as fasta:\n",
    "    for name in fasta.references:\n",
    "        chromname.append(name)\n",
    "        length = fasta.get_reference_length(name)\n",
    "        chromlength.append(length)\n",
    "        print(f\"Chromosome {name} has length {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "897d7ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to chromosome lengths file\n",
    "output_file = \"all_chrom_len.txt\"\n",
    "# Open the file in write mode\n",
    "with open(output_file, \"w\") as file:\n",
    "    # Zip the two vectors together and write them to the file\n",
    "    for n, l in zip(chromname, chromlength):\n",
    "        file.write(f\"{n}\\t{l}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05197ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to chromosome lengths file\n",
    "output_file = \"all_chrom_names.txt\"\n",
    "# Open the file in write mode\n",
    "with open(output_file, \"w\") as file:\n",
    "    # Zip the two vectors together and write them to the file\n",
    "    for n, l in zip(chromname, chromlength):\n",
    "        file.write(f\"{n}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf07ebdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>880</td>\n",
       "      <td>920</td>\n",
       "      <td>[1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>493</td>\n",
       "      <td>533</td>\n",
       "      <td>[1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr2</td>\n",
       "      <td>390</td>\n",
       "      <td>430</td>\n",
       "      <td>[2.200000047683716, 3.200000047683716]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr1</td>\n",
       "      <td>92</td>\n",
       "      <td>132</td>\n",
       "      <td>[1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>458</td>\n",
       "      <td>498</td>\n",
       "      <td>[1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>chr3</td>\n",
       "      <td>563</td>\n",
       "      <td>603</td>\n",
       "      <td>[2.0, 2.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>chr1</td>\n",
       "      <td>319</td>\n",
       "      <td>359</td>\n",
       "      <td>[1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>chr2</td>\n",
       "      <td>477</td>\n",
       "      <td>517</td>\n",
       "      <td>[3.200000047683716, 3.200000047683716]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>chr2</td>\n",
       "      <td>21</td>\n",
       "      <td>61</td>\n",
       "      <td>[0.800000011920929, 0.800000011920929]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>chr1</td>\n",
       "      <td>752</td>\n",
       "      <td>792</td>\n",
       "      <td>[1.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2                                       3\n",
       "0   chr1  880  920                              [1.0, 1.0]\n",
       "1   chr1  493  533                              [1.0, 1.0]\n",
       "2   chr2  390  430  [2.200000047683716, 3.200000047683716]\n",
       "3   chr1   92  132                              [1.0, 1.0]\n",
       "4   chr1  458  498                              [1.0, 1.0]\n",
       "..   ...  ...  ...                                     ...\n",
       "95  chr3  563  603                              [2.0, 2.0]\n",
       "96  chr1  319  359                              [1.0, 1.0]\n",
       "97  chr2  477  517  [3.200000047683716, 3.200000047683716]\n",
       "98  chr2   21   61  [0.800000011920929, 0.800000011920929]\n",
       "99  chr1  752  792                              [1.0, 1.0]\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's import the .bed file from enformer_loader\n",
    "path = 'tests/data/'\n",
    "bed_df = pd.read_csv(path + 'test_dataset.bed', sep='\\t', header=None)\n",
    "bed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d20553b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hg38_1', 'hg38_10', 'hg38_11', 'hg38_12', 'hg38_13', 'hg38_14',\n",
       "       'hg38_15', 'hg38_16', 'hg38_17', 'hg38_18', 'hg38_19', 'hg38_2',\n",
       "       'hg38_20', 'hg38_21', 'hg38_22', 'hg38_3', 'hg38_4', 'hg38_5',\n",
       "       'hg38_6', 'hg38_7', 'hg38_8', 'hg38_9', 'hg38_GL000009.2',\n",
       "       'hg38_GL000194.1', 'hg38_GL000195.1', 'hg38_GL000205.2',\n",
       "       'hg38_GL000213.1', 'hg38_GL000216.2', 'hg38_GL000218.1',\n",
       "       'hg38_GL000219.1', 'hg38_GL000220.1', 'hg38_GL000225.1',\n",
       "       'hg38_KI270442.1', 'hg38_KI270711.1', 'hg38_KI270713.1',\n",
       "       'hg38_KI270721.1', 'hg38_KI270726.1', 'hg38_KI270727.1',\n",
       "       'hg38_KI270728.1', 'hg38_KI270731.1', 'hg38_KI270733.1',\n",
       "       'hg38_KI270734.1', 'hg38_KI270744.1', 'hg38_KI270750.1', 'hg38_MT',\n",
       "       'hg38_X', 'hg38_Y', 'gg6_1'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how we could generate a train/val/test dataset for enformer\n",
    "gtf_file = \"/lustre/scratch126/gengen/projects/graft/Dataset/reference/hg38_galGal6_chr1/gtf/GRCh38.GRCg6a.chr1.110.gtf\"\n",
    "gtf_df = pd.read_csv(gtf_file, sep=\"\\t\", comment=\"#\", header=None)\n",
    "# Give appropriate column names\n",
    "gtf_df.columns = ['seqname', 'source', 'feature', 'start', 'end', '.', 'strand', '.', 'attributes']\n",
    "gtf_df['seqname'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2de73239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqname</th>\n",
       "      <th>source</th>\n",
       "      <th>feature</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>.</th>\n",
       "      <th>strand</th>\n",
       "      <th>.</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>havana</td>\n",
       "      <td>gene</td>\n",
       "      <td>11869</td>\n",
       "      <td>14409</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>gene_id \"ENSG00000290825\"; gene_version \"1\"; g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>havana</td>\n",
       "      <td>transcript</td>\n",
       "      <td>11869</td>\n",
       "      <td>14409</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>gene_id \"ENSG00000290825\"; gene_version \"1\"; t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>havana</td>\n",
       "      <td>exon</td>\n",
       "      <td>11869</td>\n",
       "      <td>12227</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>gene_id \"ENSG00000290825\"; gene_version \"1\"; t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>havana</td>\n",
       "      <td>exon</td>\n",
       "      <td>12010</td>\n",
       "      <td>12057</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>gene_id \"ENSG00000223972\"; gene_version \"6\"; t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>havana</td>\n",
       "      <td>transcript</td>\n",
       "      <td>12010</td>\n",
       "      <td>13670</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>gene_id \"ENSG00000223972\"; gene_version \"6\"; t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>havana</td>\n",
       "      <td>gene</td>\n",
       "      <td>12010</td>\n",
       "      <td>13670</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>gene_id \"ENSG00000223972\"; gene_version \"6\"; g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>havana</td>\n",
       "      <td>exon</td>\n",
       "      <td>12179</td>\n",
       "      <td>12227</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>gene_id \"ENSG00000223972\"; gene_version \"6\"; t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>havana</td>\n",
       "      <td>exon</td>\n",
       "      <td>12613</td>\n",
       "      <td>12721</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>gene_id \"ENSG00000290825\"; gene_version \"1\"; t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>havana</td>\n",
       "      <td>exon</td>\n",
       "      <td>12613</td>\n",
       "      <td>12697</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>gene_id \"ENSG00000223972\"; gene_version \"6\"; t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>havana</td>\n",
       "      <td>exon</td>\n",
       "      <td>12975</td>\n",
       "      <td>13052</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>gene_id \"ENSG00000223972\"; gene_version \"6\"; t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  seqname  source     feature  start    end  . strand  .  \\\n",
       "0  hg38_1  havana        gene  11869  14409  .      +  .   \n",
       "1  hg38_1  havana  transcript  11869  14409  .      +  .   \n",
       "2  hg38_1  havana        exon  11869  12227  .      +  .   \n",
       "3  hg38_1  havana        exon  12010  12057  .      +  .   \n",
       "4  hg38_1  havana  transcript  12010  13670  .      +  .   \n",
       "5  hg38_1  havana        gene  12010  13670  .      +  .   \n",
       "6  hg38_1  havana        exon  12179  12227  .      +  .   \n",
       "7  hg38_1  havana        exon  12613  12721  .      +  .   \n",
       "8  hg38_1  havana        exon  12613  12697  .      +  .   \n",
       "9  hg38_1  havana        exon  12975  13052  .      +  .   \n",
       "\n",
       "                                          attributes  \n",
       "0  gene_id \"ENSG00000290825\"; gene_version \"1\"; g...  \n",
       "1  gene_id \"ENSG00000290825\"; gene_version \"1\"; t...  \n",
       "2  gene_id \"ENSG00000290825\"; gene_version \"1\"; t...  \n",
       "3  gene_id \"ENSG00000223972\"; gene_version \"6\"; t...  \n",
       "4  gene_id \"ENSG00000223972\"; gene_version \"6\"; t...  \n",
       "5  gene_id \"ENSG00000223972\"; gene_version \"6\"; g...  \n",
       "6  gene_id \"ENSG00000223972\"; gene_version \"6\"; t...  \n",
       "7  gene_id \"ENSG00000290825\"; gene_version \"1\"; t...  \n",
       "8  gene_id \"ENSG00000223972\"; gene_version \"6\"; t...  \n",
       "9  gene_id \"ENSG00000223972\"; gene_version \"6\"; t...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtf_df.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dbf298c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3629944, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af60c79",
   "metadata": {},
   "source": [
    "# Now let's try to create dataset for H3K4me3, with hg38_1 as train and hg38_2 as validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3054a9",
   "metadata": {},
   "source": [
    "We have test_1_valchr.txt containing the name of the validation chromosome (hg38_2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c180cc29",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# this is the code to create our toy dataset (hg38_1 train, hg38_2 test)\n",
    "\n",
    "# python enformer_loader/scripts/generate_dataset.py --help\n",
    "\n",
    "python enformer_loader/scripts/generate_train_val_dataset.py \\\n",
    "    tests/data/chrom_sizes.txt tests/data/test.bw 40 10 \\\n",
    "    tests/data/test_train_dataset.bed tests/data/test_val_dataset.bed \\\n",
    "    tests/data/val_chroms.txt tests/data/exclude_chroms.txt \\\n",
    "    --n_bins 4 --bin_size 20 --padding 20 --seed 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f117b7d",
   "metadata": {},
   "source": [
    "- chrom_sizes: all_chrom_len.txt\n",
    "- test.bw: /lustre/scratch126/gengen/teams/parts/sb79/final/bw/merged_H3K4me3_5c3_bamcov_50.bw\n",
    "- out: test_train_dataset.bed / test_val_dataset.bed\n",
    "- val_chroms.txt: test_1_valchr.txt\n",
    "- exclude_chroms: test_exclude_chrom.txt (no hg38_1, hg38_2 BUT INCLUDE gg6_1 as we need it for test)\n",
    "- options as default - 40 train sequences, 10 val sequences"
   ]
  },
  {
   "cell_type": "raw",
   "id": "504d9a36",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "python enformer_loader/scripts/generate_train_val_dataset.py tests/data/all_chrom_len.txt /\n",
    "lustre/scratch126/gengen/teams/parts/sb79/final/bw/merged_H3K4me3_5c3_bamcov_50.bw 40 10 tests/data/test1_train_dataset.bed /\n",
    "tests/data/test1_val_dataset.bed tests/data/test_1_valchr.txt tests/data/test_exclude_chrom.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2fd1b",
   "metadata": {},
   "source": [
    "we get test1_train_dataset.bed and test1_test_dataset.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "215b0d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>144671580</td>\n",
       "      <td>144786268</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>185030105</td>\n",
       "      <td>185144793</td>\n",
       "      <td>[0.015330285954405554, 0.00879942998290062, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>36509554</td>\n",
       "      <td>36624242</td>\n",
       "      <td>[0.0, 0.008249483944382519, 0.0107243291276972...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>165722550</td>\n",
       "      <td>165837238</td>\n",
       "      <td>[0.0034372773370705545, 0.0, 0.0, 0.0273607222...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>246758040</td>\n",
       "      <td>246872728</td>\n",
       "      <td>[0.010999302758136764, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>48426077</td>\n",
       "      <td>48540765</td>\n",
       "      <td>[0.03856625911430456, 0.009280652084271424, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>217497970</td>\n",
       "      <td>217612658</td>\n",
       "      <td>[0.037122593785170466, 0.013749139907304198, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>197777256</td>\n",
       "      <td>197891944</td>\n",
       "      <td>[0.05458395095774904, 0.05582137260353193, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>215382076</td>\n",
       "      <td>215496764</td>\n",
       "      <td>[0.008524463657522574, 0.04399717366322875, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>189487854</td>\n",
       "      <td>189602542</td>\n",
       "      <td>[0.011274294723989442, 0.006599577987799421, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>111991387</td>\n",
       "      <td>112106075</td>\n",
       "      <td>[0.15014023944968358, 0.4024360461626202, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>93450435</td>\n",
       "      <td>93565123</td>\n",
       "      <td>[0.18183178477920592, 0.08476311253616586, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>246754888</td>\n",
       "      <td>246869576</td>\n",
       "      <td>[0.010724329127697274, 0.011549277522135526, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>163926650</td>\n",
       "      <td>164041338</td>\n",
       "      <td>[0.006874569953652099, 0.002474840555805713, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>230075724</td>\n",
       "      <td>230190412</td>\n",
       "      <td>[0.0175988721603062, 0.025160884048091248, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>32989172</td>\n",
       "      <td>33103860</td>\n",
       "      <td>[0.024060937721515074, 0.06613316969014704, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>245016082</td>\n",
       "      <td>245130770</td>\n",
       "      <td>[0.025298365420894697, 0.0536215040483512, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>85294697</td>\n",
       "      <td>85409385</td>\n",
       "      <td>[0.0, 0.0, 0.006874569953652099, 0.02186109461...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>4629188</td>\n",
       "      <td>4743876</td>\n",
       "      <td>[0.0021998623851686716, 0.011549277522135526, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>44972366</td>\n",
       "      <td>45087054</td>\n",
       "      <td>[0.0, 0.006462088116677478, 0.0141616064647678...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>108191576</td>\n",
       "      <td>108306264</td>\n",
       "      <td>[0.01196174041251652, 0.007837002107407898, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>239680815</td>\n",
       "      <td>239795503</td>\n",
       "      <td>[0.053827751748031005, 0.024404664363828488, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>86812387</td>\n",
       "      <td>86927075</td>\n",
       "      <td>[0.003093550694757141, 0.0175301206036238, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>26069489</td>\n",
       "      <td>26184177</td>\n",
       "      <td>[0.028254431221284904, 0.004537216169410385, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>12459826</td>\n",
       "      <td>12574514</td>\n",
       "      <td>[0.010174363531405106, 0.0, 0.0006874554674141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>152637741</td>\n",
       "      <td>152752429</td>\n",
       "      <td>[0.016911435712245293, 0.002131111948983744, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>168947186</td>\n",
       "      <td>169061874</td>\n",
       "      <td>[0.01567398165934719, 0.004949690366629511, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>227376896</td>\n",
       "      <td>227491584</td>\n",
       "      <td>[0.010174363531405106, 0.0035747763758990914, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>230764628</td>\n",
       "      <td>230879316</td>\n",
       "      <td>[0.030660497694043443, 0.04220974905183539, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>100793776</td>\n",
       "      <td>100908464</td>\n",
       "      <td>[0.07685768796363845, 0.019111260975478217, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>64718332</td>\n",
       "      <td>64833020</td>\n",
       "      <td>[0.04963425506139174, 0.03354779275832698, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>203899510</td>\n",
       "      <td>204014198</td>\n",
       "      <td>[0.0, 0.0181488256494049, 0.028735640429658815...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>61752428</td>\n",
       "      <td>61867116</td>\n",
       "      <td>[0.022136086568934843, 0.0064620813936926425, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>201975312</td>\n",
       "      <td>202090000</td>\n",
       "      <td>[0.014711565047036856, 0.01869877881836146, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>154444421</td>\n",
       "      <td>154559109</td>\n",
       "      <td>[0.01904253505927045, 0.025023413807502948, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>162939824</td>\n",
       "      <td>163054512</td>\n",
       "      <td>[0.048259384668199345, 0.016498937038704753, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>151859826</td>\n",
       "      <td>151974514</td>\n",
       "      <td>[0.04138483476708643, 0.015399031806737185, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>1620614</td>\n",
       "      <td>1735302</td>\n",
       "      <td>[0.06159601075341925, 0.012924170063342899, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>199298442</td>\n",
       "      <td>199413130</td>\n",
       "      <td>[0.0, 0.006737063580658287, 0.0071495448064524...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>hg38_1</td>\n",
       "      <td>71837549</td>\n",
       "      <td>71952237</td>\n",
       "      <td>[0.007149551995098591, 0.01856128961662762, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2  \\\n",
       "0   hg38_1  144671580  144786268   \n",
       "1   hg38_1  185030105  185144793   \n",
       "2   hg38_1   36509554   36624242   \n",
       "3   hg38_1  165722550  165837238   \n",
       "4   hg38_1  246758040  246872728   \n",
       "5   hg38_1   48426077   48540765   \n",
       "6   hg38_1  217497970  217612658   \n",
       "7   hg38_1  197777256  197891944   \n",
       "8   hg38_1  215382076  215496764   \n",
       "9   hg38_1  189487854  189602542   \n",
       "10  hg38_1  111991387  112106075   \n",
       "11  hg38_1   93450435   93565123   \n",
       "12  hg38_1  246754888  246869576   \n",
       "13  hg38_1  163926650  164041338   \n",
       "14  hg38_1  230075724  230190412   \n",
       "15  hg38_1   32989172   33103860   \n",
       "16  hg38_1  245016082  245130770   \n",
       "17  hg38_1   85294697   85409385   \n",
       "18  hg38_1    4629188    4743876   \n",
       "19  hg38_1   44972366   45087054   \n",
       "20  hg38_1  108191576  108306264   \n",
       "21  hg38_1  239680815  239795503   \n",
       "22  hg38_1   86812387   86927075   \n",
       "23  hg38_1   26069489   26184177   \n",
       "24  hg38_1   12459826   12574514   \n",
       "25  hg38_1  152637741  152752429   \n",
       "26  hg38_1  168947186  169061874   \n",
       "27  hg38_1  227376896  227491584   \n",
       "28  hg38_1  230764628  230879316   \n",
       "29  hg38_1  100793776  100908464   \n",
       "30  hg38_1   64718332   64833020   \n",
       "31  hg38_1  203899510  204014198   \n",
       "32  hg38_1   61752428   61867116   \n",
       "33  hg38_1  201975312  202090000   \n",
       "34  hg38_1  154444421  154559109   \n",
       "35  hg38_1  162939824  163054512   \n",
       "36  hg38_1  151859826  151974514   \n",
       "37  hg38_1    1620614    1735302   \n",
       "38  hg38_1  199298442  199413130   \n",
       "39  hg38_1   71837549   71952237   \n",
       "\n",
       "                                                    3  \n",
       "0   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1   [0.015330285954405554, 0.00879942998290062, 0....  \n",
       "2   [0.0, 0.008249483944382519, 0.0107243291276972...  \n",
       "3   [0.0034372773370705545, 0.0, 0.0, 0.0273607222...  \n",
       "4   [0.010999302758136764, 0.0, 0.0, 0.0, 0.0, 0.0...  \n",
       "5   [0.03856625911430456, 0.009280652084271424, 0....  \n",
       "6   [0.037122593785170466, 0.013749139907304198, 0...  \n",
       "7   [0.05458395095774904, 0.05582137260353193, 0.0...  \n",
       "8   [0.008524463657522574, 0.04399717366322875, 0....  \n",
       "9   [0.011274294723989442, 0.006599577987799421, 0...  \n",
       "10  [0.15014023944968358, 0.4024360461626202, 0.06...  \n",
       "11  [0.18183178477920592, 0.08476311253616586, 0.1...  \n",
       "12  [0.010724329127697274, 0.011549277522135526, 0...  \n",
       "13  [0.006874569953652099, 0.002474840555805713, 0...  \n",
       "14  [0.0175988721603062, 0.025160884048091248, 0.0...  \n",
       "15  [0.024060937721515074, 0.06613316969014704, 0....  \n",
       "16  [0.025298365420894697, 0.0536215040483512, 0.0...  \n",
       "17  [0.0, 0.0, 0.006874569953652099, 0.02186109461...  \n",
       "18  [0.0021998623851686716, 0.011549277522135526, ...  \n",
       "19  [0.0, 0.006462088116677478, 0.0141616064647678...  \n",
       "20  [0.01196174041251652, 0.007837002107407898, 0....  \n",
       "21  [0.053827751748031005, 0.024404664363828488, 0...  \n",
       "22  [0.003093550694757141, 0.0175301206036238, 0.0...  \n",
       "23  [0.028254431221284904, 0.004537216169410385, 0...  \n",
       "24  [0.010174363531405106, 0.0, 0.0006874554674141...  \n",
       "25  [0.016911435712245293, 0.002131111948983744, 0...  \n",
       "26  [0.01567398165934719, 0.004949690366629511, 0....  \n",
       "27  [0.010174363531405106, 0.0035747763758990914, ...  \n",
       "28  [0.030660497694043443, 0.04220974905183539, 0....  \n",
       "29  [0.07685768796363845, 0.019111260975478217, 0....  \n",
       "30  [0.04963425506139174, 0.03354779275832698, 0.0...  \n",
       "31  [0.0, 0.0181488256494049, 0.028735640429658815...  \n",
       "32  [0.022136086568934843, 0.0064620813936926425, ...  \n",
       "33  [0.014711565047036856, 0.01869877881836146, 0....  \n",
       "34  [0.01904253505927045, 0.025023413807502948, 0....  \n",
       "35  [0.048259384668199345, 0.016498937038704753, 0...  \n",
       "36  [0.04138483476708643, 0.015399031806737185, 0....  \n",
       "37  [0.06159601075341925, 0.012924170063342899, 0....  \n",
       "38  [0.0, 0.006737063580658287, 0.0071495448064524...  \n",
       "39  [0.007149551995098591, 0.01856128961662762, 0....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'tests/data/'\n",
    "bed_df = pd.read_csv(path + 'test1_train_dataset.bed', sep='\\t', header=None)\n",
    "bed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "233dfabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a25eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ast.literal_eval(bed_df.iloc[1,3])) # length is 896!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36bcac",
   "metadata": {},
   "source": [
    "## Cool, now we can get a dataset. We should be sampling enough to get enough coverage..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfaff50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40960"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "320*128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d53659b",
   "metadata": {},
   "source": [
    "let's try to load enformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0020a258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n"
     ]
    }
   ],
   "source": [
    "print('using GPU') if torch.cuda.is_available() else print('using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1287990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enformer_pytorch import Enformer, seq_indices_to_one_hot\n",
    "model = Enformer.from_hparams(\n",
    "    dim = 1536,\n",
    "    depth = 11,\n",
    "    heads = 8,\n",
    "    output_heads = dict(human = 5313, mouse = 1643),\n",
    "    target_length = 896,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "050dfd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.randint(0, 5, (1, 196_608)).cuda()\n",
    "one_hot = seq_indices_to_one_hot(seq)\n",
    "with torch.no_grad():\n",
    "    output, embeddings = model(one_hot, return_embeddings = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b141e1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.7998748 , 0.6254667 , 0.64229167, ..., 0.7191658 ,\n",
       "         0.76618105, 0.6611264 ],\n",
       "        [0.7363697 , 0.6093761 , 0.6855796 , ..., 0.7883806 ,\n",
       "         0.7830241 , 0.61660576],\n",
       "        [0.854007  , 0.5847356 , 0.72256887, ..., 0.7174225 ,\n",
       "         0.8242939 , 0.7128622 ],\n",
       "        ...,\n",
       "        [0.6512266 , 0.68517524, 0.7045935 , ..., 0.7940127 ,\n",
       "         0.73009884, 0.64410394],\n",
       "        [0.65968466, 0.73147565, 0.7095448 , ..., 0.72001946,\n",
       "         0.7941879 , 0.6191823 ],\n",
       "        [0.6820772 , 0.6101637 , 0.64959073, ..., 0.6267016 ,\n",
       "         0.7099414 , 0.6567328 ]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['human'].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64a485b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196608"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq.cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e876166c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output['human'].cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46d6ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enformer_pytorch import Enformer, seq_indices_to_one_hot, GenomeIntervalDataset\n",
    "from enformer_loader import GenomeDataIntervalDataset\n",
    "all_chroms = pd.read_csv(path + 'all_chrom_names.txt', sep='\\t', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fed6c432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hg38_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hg38_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hg38_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hg38_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hg38_13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>gg6_AADN05001564.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>gg6_AADN05001546.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>gg6_AADN05001547.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>gg6_AADN05001549.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>gg6_AADN05001562.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>658 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "0                hg38_1\n",
       "1               hg38_10\n",
       "2               hg38_11\n",
       "3               hg38_12\n",
       "4               hg38_13\n",
       "..                  ...\n",
       "653  gg6_AADN05001564.1\n",
       "654  gg6_AADN05001546.1\n",
       "655  gg6_AADN05001547.1\n",
       "656  gg6_AADN05001549.1\n",
       "657  gg6_AADN05001562.1\n",
       "\n",
       "[658 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chroms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c9e375",
   "metadata": {},
   "source": [
    "our files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b17f027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = '/lustre/scratch126/gengen/projects/graft/Dataset/reference/hg38_galGal6_full/fasta/GRCh38.GRCg6a.full.renamed.merged.fa'\n",
    "bed_file = path + 'test1_train_dataset.bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7be01de9",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 459.19 MiB is free. Including non-PyTorch memory, this process has 31.28 GiB memory in use. Of the allocated memory 30.79 GiB is allocated by PyTorch, and 129.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m ds \u001b[38;5;241m=\u001b[39m GenomeIntervalDataset(\n\u001b[1;32m      2\u001b[0m     bed_file \u001b[38;5;241m=\u001b[39m bed_file,                       \u001b[38;5;66;03m# bed file - columns 0, 1, 2 must be <chromosome>, <start position>, <end position>\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     fasta_file \u001b[38;5;241m=\u001b[39m fasta_file,                        \u001b[38;5;66;03m# path to fasta file                      # filter dataframe function\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     return_seq_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,                          \u001b[38;5;66;03m# return nucleotide indices (ACGTN) or one hot encodings\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     context_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m196_608\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m seq \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;66;03m# (196608,)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# if head = 'human', (896, 5313)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/enformer_pytorch/finetune.py:166\u001b[0m, in \u001b[0;36mHeadAdapterWrapper.forward\u001b[0;34m(self, seq, target, freeze_enformer, finetune_enformer_ln_only, finetune_last_n_layers_only)\u001b[0m\n\u001b[1;32m    164\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menformer(seq, return_only_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menformer_kwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_enformer_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfreeze_enformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_layernorms_only\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfinetune_enformer_ln_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_last_n_layers_only\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfinetune_last_n_layers_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menformer_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menformer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tracks(embeddings)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(target):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/enformer_pytorch/finetune.py:86\u001b[0m, in \u001b[0;36mget_enformer_embeddings\u001b[0;34m(model, seq, freeze, train_layernorms_only, train_last_n_layers_only, enformer_kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m enformer_context \u001b[38;5;241m=\u001b[39m null_context() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m freeze \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m enformer_context:\n\u001b[0;32m---> 86\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menformer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m freeze:\n\u001b[1;32m     89\u001b[0m         embeddings\u001b[38;5;241m.\u001b[39mdetach_()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/enformer_pytorch/modeling_enformer.py:462\u001b[0m, in \u001b[0;36mEnformer.forward\u001b[0;34m(self, x, target, return_corr_coef, return_embeddings, return_only_embeddings, head, target_length)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_target_length(target_length)\n\u001b[1;32m    461\u001b[0m trunk_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrunk_checkpointed \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_checkpointing \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trunk\n\u001b[0;32m--> 462\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtrunk_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_batch:\n\u001b[1;32m    465\u001b[0m     x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m() ... -> ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/enformer_pytorch/modeling_enformer.py:428\u001b[0m, in \u001b[0;36mEnformer.trunk_checkpointed\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrunk_checkpointed\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    427\u001b[0m     x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n d -> b d n\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 428\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_tower(x)\n\u001b[1;32m    430\u001b[0m     x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb d n -> b n d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/enformer_pytorch/modeling_enformer.py:191\u001b[0m, in \u001b[0;36mAttentionPool.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    187\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_fn(mask), mask_value)\n\u001b[1;32m    189\u001b[0m attn \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msoftmax(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn\u001b[49m)\u001b[38;5;241m.\u001b[39msum(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 459.19 MiB is free. Including non-PyTorch memory, this process has 31.28 GiB memory in use. Of the allocated memory 30.79 GiB is allocated by PyTorch, and 129.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "ds = GenomeIntervalDataset(\n",
    "    bed_file = bed_file,                       # bed file - columns 0, 1, 2 must be <chromosome>, <start position>, <end position>\n",
    "    fasta_file = fasta_file,                        # path to fasta file                      # filter dataframe function\n",
    "    return_seq_indices = True,                          # return nucleotide indices (ACGTN) or one hot encodings\n",
    "    context_length = 196_608,\n",
    ")\n",
    "\n",
    "seq = ds[2] # (196608,)\n",
    "pred = model(seq.cuda()) # if head = 'human', (896, 5313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a16048da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(896, 5313)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prednp = pred.cpu().detach().numpy()\n",
    "prednp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b9b81f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7633821 , 0.78852516, 0.7381154 , 0.7769616 , 0.70043486,\n",
       "       0.7532986 , 0.788241  , 0.7081002 , 0.81874466, 0.6937693 ,\n",
       "       0.67318714, 0.8322517 , 0.76112586, 0.78210145, 0.7183264 ,\n",
       "       0.7139891 , 0.65781593, 0.79023933, 0.7460313 , 0.6592545 ,\n",
       "       0.66798127, 0.6463968 , 0.75490975, 0.81654584, 0.59866005,\n",
       "       0.76725906, 0.71866643, 0.7153028 , 0.85849726, 0.7743673 ,\n",
       "       0.78089   , 0.73537904, 0.7355636 , 0.7544934 , 0.6835425 ,\n",
       "       0.77686524, 0.8376123 , 0.80208534, 0.75630915, 0.693698  ,\n",
       "       0.58794093, 0.7600564 , 0.7194239 , 0.68696666, 0.67759   ,\n",
       "       0.8083098 , 0.6653601 , 0.74387926, 0.68167895, 0.7879913 ,\n",
       "       0.7986345 , 0.7195951 , 0.7617609 , 0.75024253, 0.72655493,\n",
       "       0.75230277, 0.6875209 , 0.73683256, 0.6927696 , 0.7079625 ,\n",
       "       0.75102514, 0.77746034, 0.7436395 , 0.859849  , 0.6949821 ,\n",
       "       0.8701673 , 0.733235  , 0.6686225 , 0.70567435, 0.7223281 ,\n",
       "       0.7641489 , 0.74396557, 0.76451135, 0.68999094, 0.7290143 ,\n",
       "       0.80847937, 0.7165495 , 0.74881303, 0.7410992 , 0.8512392 ,\n",
       "       0.6474142 , 0.7154242 , 0.68317103, 0.76349664, 0.782741  ,\n",
       "       0.8476522 , 0.6630804 , 0.7352134 , 0.7174466 , 0.8334553 ,\n",
       "       0.6008649 , 0.81115556, 0.7050061 , 0.73427904, 0.7186489 ,\n",
       "       0.7256505 , 0.7601702 , 0.825691  , 0.7412101 , 0.7718753 ,\n",
       "       0.7845396 , 0.699849  , 0.6761546 , 0.6983319 , 0.6788995 ,\n",
       "       0.774363  , 0.8014344 , 0.66964144, 0.660101  , 0.78070337,\n",
       "       0.6390267 , 0.79969394, 0.8021834 , 0.84417105, 0.7245118 ,\n",
       "       0.75360954, 0.7971353 , 0.68104994, 0.76548964, 0.780389  ,\n",
       "       0.6587329 , 0.82323   , 0.8330186 , 0.7682404 , 0.77204716,\n",
       "       0.7001221 , 0.7086261 , 0.7879973 , 0.71163666, 0.6686569 ,\n",
       "       0.71976894, 0.74789596, 0.72119504, 0.72222215, 0.75463146,\n",
       "       0.73739254, 0.6947299 , 0.66628957, 0.7344936 , 0.7474284 ,\n",
       "       0.7049006 , 0.69876856, 0.72933   , 0.72323364, 0.7572791 ,\n",
       "       0.77148676, 0.79425657, 0.74967635, 0.7780603 , 0.8830063 ,\n",
       "       0.6686226 , 0.7225401 , 0.79015964, 0.8306434 , 0.71173805,\n",
       "       0.7420741 , 0.7898065 , 0.74219584, 0.77116984, 0.6466466 ,\n",
       "       0.79189616, 0.7453563 , 0.6853394 , 0.741688  , 0.5203625 ,\n",
       "       0.76884264, 0.73254484, 0.6916475 , 0.7642672 , 0.70705676,\n",
       "       0.63624716, 0.8072833 , 0.7279288 , 0.8338564 , 0.6940245 ,\n",
       "       0.747667  , 0.72214645, 0.6500771 , 0.6793603 , 0.8343298 ,\n",
       "       0.79128206, 0.6992328 , 0.7806828 , 0.6397118 , 0.7761175 ,\n",
       "       0.6742827 , 0.78144246, 0.6966509 , 0.6970224 , 0.7547855 ,\n",
       "       0.66222847, 0.79304785, 0.7459046 , 0.6831857 , 0.7141309 ,\n",
       "       0.64325374, 0.70072967, 0.7602159 , 0.70950866, 0.67283314,\n",
       "       0.7324498 , 0.8321315 , 0.71659315, 0.8378972 , 0.72767544,\n",
       "       0.7609177 , 0.7055033 , 0.71486336, 0.68099916, 0.7329268 ,\n",
       "       0.7204391 , 0.6898008 , 0.80254906, 0.70865035, 0.78236777,\n",
       "       0.705705  , 0.72625613, 0.63923144, 0.6996695 , 0.6850469 ,\n",
       "       0.7700146 , 0.673133  , 0.68511075, 0.80823517, 0.8083431 ,\n",
       "       0.67675483, 0.79714143, 0.745686  , 0.7622462 , 0.7201652 ,\n",
       "       0.75352436, 0.66825765, 0.7669973 , 0.70299983, 0.7819193 ,\n",
       "       0.70138276, 0.7397107 , 0.72633356, 0.77819765, 0.7844379 ,\n",
       "       0.71350217, 0.66155344, 0.6641514 , 0.6697384 , 0.8683607 ,\n",
       "       0.81266487, 0.8039731 , 0.6873229 , 0.59143114, 0.68364716,\n",
       "       0.7659716 , 0.6644972 , 0.6107014 , 0.7133305 , 0.63774514,\n",
       "       0.7440228 , 0.68457997, 0.6848532 , 0.65916175, 0.71245974,\n",
       "       0.8120905 , 0.7319222 , 0.6679245 , 0.7400458 , 0.80054355,\n",
       "       0.78897953, 0.69343394, 0.7679775 , 0.7802564 , 0.7749737 ,\n",
       "       0.70177466, 0.7225247 , 0.81854546, 0.8050539 , 0.6391143 ,\n",
       "       0.73273885, 0.8026452 , 0.7764983 , 0.68824804, 0.66126245,\n",
       "       0.7526424 , 0.7662065 , 0.85689336, 0.7075527 , 0.7542745 ,\n",
       "       0.682942  , 0.72151685, 0.7315401 , 0.6978297 , 0.93640995,\n",
       "       0.7046793 , 0.66522723, 0.63114566, 0.7575766 , 0.6777205 ,\n",
       "       0.85908294, 0.7139747 , 0.68091995, 0.7302333 , 0.7104995 ,\n",
       "       0.7869049 , 0.608458  , 0.78370607, 0.8078274 , 0.6587806 ,\n",
       "       0.70916873, 0.69467795, 0.70454377, 0.71400076, 0.725312  ,\n",
       "       0.7391096 , 0.68799514, 0.8188684 , 0.7553542 , 0.7513689 ,\n",
       "       0.88563865, 0.73065764, 0.6937461 , 0.7385985 , 0.723071  ,\n",
       "       0.81568795, 0.71838427, 0.711367  , 0.7443622 , 0.7354095 ,\n",
       "       0.71547455, 0.61982745, 0.73055327, 0.80805826, 0.76498914,\n",
       "       0.6873204 , 0.7312795 , 0.78180134, 0.68154377, 0.7389758 ,\n",
       "       0.85720193, 0.7279978 , 0.6357941 , 0.7378365 , 0.73754466,\n",
       "       0.6257985 , 0.7061212 , 0.6992916 , 0.7771346 , 0.6766669 ,\n",
       "       0.77205145, 0.7213257 , 0.75058985, 0.721502  , 0.6177485 ,\n",
       "       0.7401937 , 0.8223126 , 0.8255639 , 0.7164168 , 0.6709475 ,\n",
       "       0.85446966, 0.7436583 , 0.8749574 , 0.838996  , 0.7377439 ,\n",
       "       0.7419851 , 0.7277497 , 0.6633966 , 0.7046599 , 0.74320656,\n",
       "       0.7877222 , 0.79020035, 0.79797447, 0.7339417 , 0.8161762 ,\n",
       "       0.86566275, 0.71347624, 0.64145714, 0.64700043, 0.74755377,\n",
       "       0.6467792 , 0.7534317 , 0.71936226, 0.64043504, 0.7362176 ,\n",
       "       0.6224482 , 0.6716637 , 0.74339545, 0.73418653, 0.6939306 ,\n",
       "       0.7522732 , 0.7969759 , 0.7021218 , 0.7295768 , 0.74615556,\n",
       "       0.8656325 , 0.6301514 , 0.83401203, 0.67395777, 0.6196332 ,\n",
       "       0.70074075, 0.7644288 , 0.8102318 , 0.5755885 , 0.78873396,\n",
       "       0.666068  , 0.7844883 , 0.8614232 , 0.7666238 , 0.6762843 ,\n",
       "       0.7465051 , 0.6555409 , 0.7316566 , 0.70777947, 0.76673424,\n",
       "       0.76815295, 0.60765034, 0.7713888 , 0.77772105, 0.6815388 ,\n",
       "       0.7287712 , 0.7099754 , 0.73474586, 0.7238786 , 0.79795665,\n",
       "       0.7771449 , 0.7074556 , 0.7626697 , 0.77224046, 0.7133496 ,\n",
       "       0.6865485 , 0.71414167, 0.7384533 , 0.66206443, 0.7579022 ,\n",
       "       0.7247678 , 0.70998585, 0.76051676, 0.629559  , 0.6614195 ,\n",
       "       0.6125558 , 0.79490626, 0.79142594, 0.71979517, 0.682873  ,\n",
       "       0.8211423 , 0.85796016, 0.78113496, 0.8192056 , 0.7246338 ,\n",
       "       0.781607  , 0.7431442 , 0.6726638 , 0.6276463 , 0.7353979 ,\n",
       "       0.72654605, 0.7394354 , 0.6295325 , 0.6511614 , 0.6755198 ,\n",
       "       0.7843092 , 0.6576787 , 0.6841908 , 0.7664486 , 0.64839005,\n",
       "       0.7712411 , 0.8384345 , 0.74869776, 0.7139755 , 0.7269879 ,\n",
       "       0.64279395, 0.7092343 , 0.7050803 , 0.84408116, 0.7404642 ,\n",
       "       0.7025422 , 0.65976727, 0.7870576 , 0.7420468 , 0.78042483,\n",
       "       0.7550514 , 0.75004303, 0.7430619 , 0.8128083 , 0.73756987,\n",
       "       0.73566985, 0.7110068 , 0.8525697 , 0.648093  , 0.67199445,\n",
       "       0.67985225, 0.7950373 , 0.681442  , 0.702959  , 0.6809626 ,\n",
       "       0.7131637 , 0.78017026, 0.7314245 , 0.73730546, 0.6433908 ,\n",
       "       0.7560738 , 0.7807749 , 0.8686979 , 0.7422271 , 0.62206393,\n",
       "       0.76395255, 0.7536327 , 0.8072324 , 0.84843856, 0.7067037 ,\n",
       "       0.61302066, 0.6654819 , 0.8547859 , 0.79471755, 0.7422303 ,\n",
       "       0.7389655 , 0.79535234, 0.6568207 , 0.6555232 , 0.6152288 ,\n",
       "       0.6726258 , 0.6784974 , 0.7477337 , 0.648707  , 0.57136834,\n",
       "       0.6597564 , 0.7577103 , 0.8040468 , 0.69683486, 0.8223543 ,\n",
       "       0.69172204, 0.6561086 , 0.78239465, 0.73305005, 0.79523516,\n",
       "       0.7586823 , 0.66694725, 0.67942446, 0.7784839 , 0.7191318 ,\n",
       "       0.75675523, 0.7135219 , 0.7165804 , 0.76182944, 0.7385713 ,\n",
       "       0.67035997, 0.744764  , 0.79932857, 0.8550333 , 0.7777813 ,\n",
       "       0.742427  , 0.77865523, 0.69186217, 0.6639693 , 0.74291164,\n",
       "       0.79577374, 0.7171601 , 0.80273324, 0.7620579 , 0.7631988 ,\n",
       "       0.68014544, 0.69089556, 0.7019993 , 0.69525844, 0.70894045,\n",
       "       0.730351  , 0.6958275 , 0.74584633, 0.77401096, 0.6828331 ,\n",
       "       0.7458206 , 0.7325565 , 0.7246082 , 0.7004671 , 0.81503236,\n",
       "       0.84512776, 0.5900651 , 0.6575856 , 0.7601097 , 0.6990208 ,\n",
       "       0.77378124, 0.68503815, 0.7327682 , 0.726933  , 0.64625347,\n",
       "       0.7496105 , 0.7489823 , 0.72309446, 0.7105785 , 0.860524  ,\n",
       "       0.7579012 , 0.70139706, 0.8034379 , 0.79077995, 0.8061547 ,\n",
       "       0.76914257, 0.85503507, 0.7425339 , 0.6283901 , 0.7664253 ,\n",
       "       0.7110194 , 0.6693501 , 0.84577626, 0.6574786 , 0.7004091 ,\n",
       "       0.6864129 , 0.6877675 , 0.6774011 , 0.8145221 , 0.6916248 ,\n",
       "       0.7962155 , 0.64687604, 0.6783834 , 0.60735357, 0.81506664,\n",
       "       0.6696374 , 0.8770733 , 0.7041373 , 0.7578132 , 0.6076305 ,\n",
       "       0.78849894, 0.8359539 , 0.8046721 , 0.7253694 , 0.79210764,\n",
       "       0.7026716 , 0.7079909 , 0.82887745, 0.7086735 , 0.7498502 ,\n",
       "       0.6986512 , 0.6522807 , 0.784944  , 0.7438352 , 0.7354326 ,\n",
       "       0.63492674, 0.76827395, 0.78385836, 0.6636175 , 0.85141766,\n",
       "       0.7448579 , 0.71759254, 0.67218363, 0.78770447, 0.72627765,\n",
       "       0.71018857, 0.71884733, 0.62969905, 0.7119515 , 0.7391106 ,\n",
       "       0.66457134, 0.6955723 , 0.8311386 , 0.8059114 , 0.7350048 ,\n",
       "       0.65014035, 0.75026023, 0.7018665 , 0.7558918 , 0.66200876,\n",
       "       0.7916265 , 0.7672584 , 0.72007024, 0.8733293 , 0.68856496,\n",
       "       0.7891506 , 0.84011275, 0.67212075, 0.6961171 , 0.6999459 ,\n",
       "       0.74571717, 0.627885  , 0.75917566, 0.74020153, 0.72562855,\n",
       "       0.74800926, 0.71459997, 0.7108889 , 0.7451048 , 0.6777148 ,\n",
       "       0.72923714, 0.75564516, 0.73481417, 0.7319663 , 0.8005074 ,\n",
       "       0.7171357 , 0.7094935 , 0.6638273 , 0.72583735, 0.7182059 ,\n",
       "       0.657647  , 0.8068362 , 0.79083043, 0.69367903, 0.66347855,\n",
       "       0.8126052 , 0.7055105 , 0.78223836, 0.69308484, 0.6264897 ,\n",
       "       0.6770304 , 0.69412595, 0.8875094 , 0.7445373 , 0.78953916,\n",
       "       0.6226377 , 0.67146045, 0.73532355, 0.7113707 , 0.801784  ,\n",
       "       0.8447666 , 0.72548693, 0.68037206, 0.66392237, 0.8023998 ,\n",
       "       0.6995439 , 0.75607896, 0.7272837 , 0.73212546, 0.652174  ,\n",
       "       0.735022  , 0.70925254, 0.72052747, 0.81473905, 0.73426276,\n",
       "       0.6504596 , 0.7612914 , 0.75044745, 0.76389056, 0.7082861 ,\n",
       "       0.70611364, 0.7376771 , 0.7234024 , 0.68668395, 0.80263513,\n",
       "       0.71111995, 0.7263481 , 0.78438   , 0.67380136, 0.7963989 ,\n",
       "       0.6571909 , 0.7053707 , 0.63639313, 0.7183101 , 0.698431  ,\n",
       "       0.70358455, 0.7257557 , 0.6693342 , 0.69958365, 0.6388046 ,\n",
       "       0.74419576, 0.7137999 , 0.67101216, 0.81552714, 0.7648672 ,\n",
       "       0.7767474 , 0.73006445, 0.72171867, 0.6714065 , 0.6923851 ,\n",
       "       0.7976928 , 0.71854293, 0.7305539 , 0.74968886, 0.7122091 ,\n",
       "       0.6918623 , 0.61932105, 0.69755864, 0.7039821 , 0.6971308 ,\n",
       "       0.89123154, 0.7196493 , 0.7750328 , 0.66429275, 0.73942244,\n",
       "       0.6546867 , 0.76240414, 0.8069001 , 0.7454492 , 0.7497756 ,\n",
       "       0.69476706, 0.70352525, 0.74061704, 0.7772825 , 0.6629602 ,\n",
       "       0.66547835, 0.83293474, 0.95772266, 0.79730153, 0.7592689 ,\n",
       "       0.69108886, 0.7528172 , 0.6961832 , 0.8523545 , 0.8094306 ,\n",
       "       0.67901635, 0.81287867, 0.64907515, 0.65810204, 0.67542154,\n",
       "       0.67128736, 0.6961225 , 0.88493514, 0.7217522 , 0.7515822 ,\n",
       "       0.77984905, 0.6758512 , 0.7333622 , 0.7637648 , 0.6693942 ,\n",
       "       0.6916126 , 0.6611426 , 0.73574036, 0.6438888 , 0.67715126,\n",
       "       0.7493493 , 0.7777162 , 0.81584436, 0.74217796, 0.727196  ,\n",
       "       0.79241735, 0.7510483 , 0.7270909 , 0.7012039 , 0.7215771 ,\n",
       "       0.764195  , 0.74486196, 0.7810533 , 0.7237563 , 0.62667996,\n",
       "       0.78771234, 0.7183883 , 0.7501582 , 0.8925519 , 0.79364836,\n",
       "       0.7050529 , 0.75117177, 0.61982036, 0.7957585 , 0.8639687 ,\n",
       "       0.6773438 , 0.7112608 , 0.72464687, 0.8353598 , 0.7298362 ,\n",
       "       0.7487912 , 0.72516805, 0.7255021 , 0.7160676 , 0.6948535 ,\n",
       "       0.6964235 , 0.72107345, 0.7712228 , 0.74962837, 0.8344204 ,\n",
       "       0.7078325 , 0.7494668 , 0.78030926, 0.60716707, 0.69221395,\n",
       "       0.6963589 , 0.6531301 , 0.71687365, 0.67250985, 0.7464041 ,\n",
       "       0.68155646, 0.7887154 , 0.7290764 , 0.7745975 , 0.8495898 ,\n",
       "       0.6729376 , 0.77360374, 0.67820954, 0.77965224, 0.6530197 ,\n",
       "       0.6114014 , 0.789815  , 0.6136458 , 0.69904923, 0.6522694 ,\n",
       "       0.69197714, 0.7817339 , 0.6289103 , 0.76895565, 0.72191274,\n",
       "       0.76493925, 0.75810397, 0.70420265, 0.6886302 , 0.6556175 ,\n",
       "       0.67999756, 0.7651446 , 0.70703447, 0.7111697 , 0.7017065 ,\n",
       "       0.6690541 , 0.72823507, 0.76963645, 0.71190083, 0.638531  ,\n",
       "       0.7064642 ], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prednp[:,1436]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e0bdfc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with Jacob's code - all_chroms is a list!\n",
    "all_chroms = pd.read_csv(path + 'all_chrom_names.txt', sep='\\t', header=None)\n",
    "allchromlist = list(all_chroms.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "94aafcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hg38_1',\n",
       " 'hg38_10',\n",
       " 'hg38_11',\n",
       " 'hg38_12',\n",
       " 'hg38_13',\n",
       " 'hg38_14',\n",
       " 'hg38_15',\n",
       " 'hg38_16',\n",
       " 'hg38_17',\n",
       " 'hg38_18',\n",
       " 'hg38_19',\n",
       " 'hg38_2',\n",
       " 'hg38_20',\n",
       " 'hg38_21',\n",
       " 'hg38_22',\n",
       " 'hg38_3',\n",
       " 'hg38_4',\n",
       " 'hg38_5',\n",
       " 'hg38_6',\n",
       " 'hg38_7',\n",
       " 'hg38_8',\n",
       " 'hg38_9',\n",
       " 'hg38_MT',\n",
       " 'hg38_X',\n",
       " 'hg38_Y',\n",
       " 'hg38_KI270728.1',\n",
       " 'hg38_KI270727.1',\n",
       " 'hg38_KI270442.1',\n",
       " 'hg38_KI270729.1',\n",
       " 'hg38_GL000225.1',\n",
       " 'hg38_KI270743.1',\n",
       " 'hg38_GL000008.2',\n",
       " 'hg38_GL000009.2',\n",
       " 'hg38_KI270747.1',\n",
       " 'hg38_KI270722.1',\n",
       " 'hg38_GL000194.1',\n",
       " 'hg38_KI270742.1',\n",
       " 'hg38_GL000205.2',\n",
       " 'hg38_GL000195.1',\n",
       " 'hg38_KI270736.1',\n",
       " 'hg38_KI270733.1',\n",
       " 'hg38_GL000224.1',\n",
       " 'hg38_GL000219.1',\n",
       " 'hg38_KI270719.1',\n",
       " 'hg38_GL000216.2',\n",
       " 'hg38_KI270712.1',\n",
       " 'hg38_KI270706.1',\n",
       " 'hg38_KI270725.1',\n",
       " 'hg38_KI270744.1',\n",
       " 'hg38_KI270734.1',\n",
       " 'hg38_GL000213.1',\n",
       " 'hg38_GL000220.1',\n",
       " 'hg38_KI270715.1',\n",
       " 'hg38_GL000218.1',\n",
       " 'hg38_KI270749.1',\n",
       " 'hg38_KI270741.1',\n",
       " 'hg38_GL000221.1',\n",
       " 'hg38_KI270716.1',\n",
       " 'hg38_KI270731.1',\n",
       " 'hg38_KI270751.1',\n",
       " 'hg38_KI270750.1',\n",
       " 'hg38_KI270519.1',\n",
       " 'hg38_GL000214.1',\n",
       " 'hg38_KI270708.1',\n",
       " 'hg38_KI270730.1',\n",
       " 'hg38_KI270438.1',\n",
       " 'hg38_KI270737.1',\n",
       " 'hg38_KI270721.1',\n",
       " 'hg38_KI270738.1',\n",
       " 'hg38_KI270748.1',\n",
       " 'hg38_KI270435.1',\n",
       " 'hg38_GL000208.1',\n",
       " 'hg38_KI270538.1',\n",
       " 'hg38_KI270756.1',\n",
       " 'hg38_KI270739.1',\n",
       " 'hg38_KI270757.1',\n",
       " 'hg38_KI270709.1',\n",
       " 'hg38_KI270746.1',\n",
       " 'hg38_KI270753.1',\n",
       " 'hg38_KI270589.1',\n",
       " 'hg38_KI270726.1',\n",
       " 'hg38_KI270735.1',\n",
       " 'hg38_KI270711.1',\n",
       " 'hg38_KI270745.1',\n",
       " 'hg38_KI270714.1',\n",
       " 'hg38_KI270732.1',\n",
       " 'hg38_KI270713.1',\n",
       " 'hg38_KI270754.1',\n",
       " 'hg38_KI270710.1',\n",
       " 'hg38_KI270717.1',\n",
       " 'hg38_KI270724.1',\n",
       " 'hg38_KI270720.1',\n",
       " 'hg38_KI270723.1',\n",
       " 'hg38_KI270718.1',\n",
       " 'hg38_KI270317.1',\n",
       " 'hg38_KI270740.1',\n",
       " 'hg38_KI270755.1',\n",
       " 'hg38_KI270707.1',\n",
       " 'hg38_KI270579.1',\n",
       " 'hg38_KI270752.1',\n",
       " 'hg38_KI270512.1',\n",
       " 'hg38_KI270322.1',\n",
       " 'hg38_GL000226.1',\n",
       " 'hg38_KI270311.1',\n",
       " 'hg38_KI270366.1',\n",
       " 'hg38_KI270511.1',\n",
       " 'hg38_KI270448.1',\n",
       " 'hg38_KI270521.1',\n",
       " 'hg38_KI270581.1',\n",
       " 'hg38_KI270582.1',\n",
       " 'hg38_KI270515.1',\n",
       " 'hg38_KI270588.1',\n",
       " 'hg38_KI270591.1',\n",
       " 'hg38_KI270522.1',\n",
       " 'hg38_KI270507.1',\n",
       " 'hg38_KI270590.1',\n",
       " 'hg38_KI270584.1',\n",
       " 'hg38_KI270320.1',\n",
       " 'hg38_KI270382.1',\n",
       " 'hg38_KI270468.1',\n",
       " 'hg38_KI270467.1',\n",
       " 'hg38_KI270362.1',\n",
       " 'hg38_KI270517.1',\n",
       " 'hg38_KI270593.1',\n",
       " 'hg38_KI270528.1',\n",
       " 'hg38_KI270587.1',\n",
       " 'hg38_KI270364.1',\n",
       " 'hg38_KI270371.1',\n",
       " 'hg38_KI270333.1',\n",
       " 'hg38_KI270374.1',\n",
       " 'hg38_KI270411.1',\n",
       " 'hg38_KI270414.1',\n",
       " 'hg38_KI270510.1',\n",
       " 'hg38_KI270390.1',\n",
       " 'hg38_KI270375.1',\n",
       " 'hg38_KI270420.1',\n",
       " 'hg38_KI270509.1',\n",
       " 'hg38_KI270315.1',\n",
       " 'hg38_KI270302.1',\n",
       " 'hg38_KI270518.1',\n",
       " 'hg38_KI270530.1',\n",
       " 'hg38_KI270304.1',\n",
       " 'hg38_KI270418.1',\n",
       " 'hg38_KI270424.1',\n",
       " 'hg38_KI270417.1',\n",
       " 'hg38_KI270508.1',\n",
       " 'hg38_KI270303.1',\n",
       " 'hg38_KI270381.1',\n",
       " 'hg38_KI270529.1',\n",
       " 'hg38_KI270425.1',\n",
       " 'hg38_KI270396.1',\n",
       " 'hg38_KI270363.1',\n",
       " 'hg38_KI270386.1',\n",
       " 'hg38_KI270465.1',\n",
       " 'hg38_KI270383.1',\n",
       " 'hg38_KI270384.1',\n",
       " 'hg38_KI270330.1',\n",
       " 'hg38_KI270372.1',\n",
       " 'hg38_KI270548.1',\n",
       " 'hg38_KI270580.1',\n",
       " 'hg38_KI270387.1',\n",
       " 'hg38_KI270391.1',\n",
       " 'hg38_KI270305.1',\n",
       " 'hg38_KI270373.1',\n",
       " 'hg38_KI270422.1',\n",
       " 'hg38_KI270316.1',\n",
       " 'hg38_KI270340.1',\n",
       " 'hg38_KI270338.1',\n",
       " 'hg38_KI270583.1',\n",
       " 'hg38_KI270334.1',\n",
       " 'hg38_KI270429.1',\n",
       " 'hg38_KI270393.1',\n",
       " 'hg38_KI270516.1',\n",
       " 'hg38_KI270389.1',\n",
       " 'hg38_KI270466.1',\n",
       " 'hg38_KI270388.1',\n",
       " 'hg38_KI270544.1',\n",
       " 'hg38_KI270310.1',\n",
       " 'hg38_KI270412.1',\n",
       " 'hg38_KI270395.1',\n",
       " 'hg38_KI270376.1',\n",
       " 'hg38_KI270337.1',\n",
       " 'hg38_KI270335.1',\n",
       " 'hg38_KI270378.1',\n",
       " 'hg38_KI270379.1',\n",
       " 'hg38_KI270329.1',\n",
       " 'hg38_KI270419.1',\n",
       " 'hg38_KI270336.1',\n",
       " 'hg38_KI270312.1',\n",
       " 'hg38_KI270539.1',\n",
       " 'hg38_KI270385.1',\n",
       " 'hg38_KI270423.1',\n",
       " 'hg38_KI270392.1',\n",
       " 'hg38_KI270394.1',\n",
       " 'gg6_MT',\n",
       " 'gg6_W',\n",
       " 'gg6_Z',\n",
       " 'gg6_1',\n",
       " 'gg6_2',\n",
       " 'gg6_3',\n",
       " 'gg6_4',\n",
       " 'gg6_5',\n",
       " 'gg6_6',\n",
       " 'gg6_7',\n",
       " 'gg6_8',\n",
       " 'gg6_9',\n",
       " 'gg6_10',\n",
       " 'gg6_11',\n",
       " 'gg6_12',\n",
       " 'gg6_13',\n",
       " 'gg6_14',\n",
       " 'gg6_15',\n",
       " 'gg6_16',\n",
       " 'gg6_17',\n",
       " 'gg6_18',\n",
       " 'gg6_19',\n",
       " 'gg6_20',\n",
       " 'gg6_21',\n",
       " 'gg6_22',\n",
       " 'gg6_23',\n",
       " 'gg6_24',\n",
       " 'gg6_25',\n",
       " 'gg6_26',\n",
       " 'gg6_27',\n",
       " 'gg6_28',\n",
       " 'gg6_30',\n",
       " 'gg6_31',\n",
       " 'gg6_32',\n",
       " 'gg6_33',\n",
       " 'gg6_KZ626839.1',\n",
       " 'gg6_KZ626837.1',\n",
       " 'gg6_KZ626838.1',\n",
       " 'gg6_KZ626835.1',\n",
       " 'gg6_KZ626836.1',\n",
       " 'gg6_KZ626833.1',\n",
       " 'gg6_KZ626834.1',\n",
       " 'gg6_KZ626831.1',\n",
       " 'gg6_KZ626832.1',\n",
       " 'gg6_KZ626829.1',\n",
       " 'gg6_KZ626819.1',\n",
       " 'gg6_KZ626830.1',\n",
       " 'gg6_KZ626827.1',\n",
       " 'gg6_KZ626828.1',\n",
       " 'gg6_AADN05001291.1',\n",
       " 'gg6_KZ626826.1',\n",
       " 'gg6_AADN05001235.1',\n",
       " 'gg6_AADN05001385.1',\n",
       " 'gg6_AADN05001349.1',\n",
       " 'gg6_AADN05001188.1',\n",
       " 'gg6_AADN05001187.1',\n",
       " 'gg6_KZ626824.1',\n",
       " 'gg6_AADN05001513.1',\n",
       " 'gg6_AADN05001353.1',\n",
       " 'gg6_AADN05000202.1',\n",
       " 'gg6_AADN05001371.1',\n",
       " 'gg6_AADN05001420.1',\n",
       " 'gg6_AADN05001193.1',\n",
       " 'gg6_AADN05001498.1',\n",
       " 'gg6_AADN05001414.1',\n",
       " 'gg6_AADN05000531.1',\n",
       " 'gg6_AADN05001370.1',\n",
       " 'gg6_AADN05000525.1',\n",
       " 'gg6_AADN05001329.1',\n",
       " 'gg6_AADN05001466.1',\n",
       " 'gg6_AADN05001240.1',\n",
       " 'gg6_AADN05001306.1',\n",
       " 'gg6_AADN05001220.1',\n",
       " 'gg6_AADN05001219.1',\n",
       " 'gg6_AADN05001361.1',\n",
       " 'gg6_AADN05001399.1',\n",
       " 'gg6_AADN05001393.1',\n",
       " 'gg6_AADN05001400.1',\n",
       " 'gg6_AADN05001292.1',\n",
       " 'gg6_AADN05001357.1',\n",
       " 'gg6_AADN05001476.1',\n",
       " 'gg6_AADN05001287.1',\n",
       " 'gg6_AADN05001464.1',\n",
       " 'gg6_AADN05001432.1',\n",
       " 'gg6_AADN05001241.1',\n",
       " 'gg6_AADN05001518.1',\n",
       " 'gg6_AADN05001504.1',\n",
       " 'gg6_AADN05001221.1',\n",
       " 'gg6_AADN05001307.1',\n",
       " 'gg6_AADN05001423.1',\n",
       " 'gg6_AADN05001288.1',\n",
       " 'gg6_AADN05001326.1',\n",
       " 'gg6_AADN05001424.1',\n",
       " 'gg6_AADN05001460.1',\n",
       " 'gg6_AADN05001222.1',\n",
       " 'gg6_AADN05001405.1',\n",
       " 'gg6_AADN05001505.1',\n",
       " 'gg6_AADN05001390.1',\n",
       " 'gg6_AADN05001172.1',\n",
       " 'gg6_AADN05001330.1',\n",
       " 'gg6_AADN05001350.1',\n",
       " 'gg6_AADN05001223.1',\n",
       " 'gg6_AADN05001224.1',\n",
       " 'gg6_AADN05001290.1',\n",
       " 'gg6_AADN05001343.1',\n",
       " 'gg6_AADN05001355.1',\n",
       " 'gg6_AADN05001145.1',\n",
       " 'gg6_AADN05001227.1',\n",
       " 'gg6_AADN05001250.1',\n",
       " 'gg6_AADN05001483.1',\n",
       " 'gg6_AADN05001467.1',\n",
       " 'gg6_AADN05001293.1',\n",
       " 'gg6_AADN05001433.1',\n",
       " 'gg6_AADN05001425.1',\n",
       " 'gg6_AADN05001275.1',\n",
       " 'gg6_AADN05001461.1',\n",
       " 'gg6_AADN05001434.1',\n",
       " 'gg6_AADN05001422.1',\n",
       " 'gg6_AADN05001482.1',\n",
       " 'gg6_AADN05001197.1',\n",
       " 'gg6_AADN05001160.1',\n",
       " 'gg6_AADN05001217.1',\n",
       " 'gg6_AADN05001202.1',\n",
       " 'gg6_AADN05001366.1',\n",
       " 'gg6_AADN05001503.1',\n",
       " 'gg6_AADN05001496.1',\n",
       " 'gg6_AADN05001416.1',\n",
       " 'gg6_AADN05001323.1',\n",
       " 'gg6_AADN05001435.1',\n",
       " 'gg6_AADN05001412.1',\n",
       " 'gg6_AADN05001478.1',\n",
       " 'gg6_AADN05001468.1',\n",
       " 'gg6_AADN05001308.1',\n",
       " 'gg6_AADN05001237.1',\n",
       " 'gg6_AADN05001506.1',\n",
       " 'gg6_AADN05001377.1',\n",
       " 'gg6_AADN05001264.1',\n",
       " 'gg6_AADN05001225.1',\n",
       " 'gg6_AADN05001436.1',\n",
       " 'gg6_AADN05001331.1',\n",
       " 'gg6_AADN05001348.1',\n",
       " 'gg6_AADN05001267.1',\n",
       " 'gg6_AADN05001497.1',\n",
       " 'gg6_AADN05001437.1',\n",
       " 'gg6_AADN05001372.1',\n",
       " 'gg6_AADN05001469.1',\n",
       " 'gg6_AADN05001378.1',\n",
       " 'gg6_AADN05001199.1',\n",
       " 'gg6_AADN05001289.1',\n",
       " 'gg6_AADN05001345.1',\n",
       " 'gg6_AADN05001312.1',\n",
       " 'gg6_AADN05001439.1',\n",
       " 'gg6_AADN05001214.1',\n",
       " 'gg6_AADN05001438.1',\n",
       " 'gg6_AADN05001233.1',\n",
       " 'gg6_AADN05001205.1',\n",
       " 'gg6_AADN05001226.1',\n",
       " 'gg6_AADN05001234.1',\n",
       " 'gg6_AADN05001271.1',\n",
       " 'gg6_AADN05001283.1',\n",
       " 'gg6_AADN05001389.1',\n",
       " 'gg6_AADN05001462.1',\n",
       " 'gg6_AADN05001191.1',\n",
       " 'gg6_AADN05001524.1',\n",
       " 'gg6_AADN05001302.1',\n",
       " 'gg6_AADN05001441.1',\n",
       " 'gg6_AADN05001440.1',\n",
       " 'gg6_AADN05001165.1',\n",
       " 'gg6_AADN05001459.1',\n",
       " 'gg6_AADN05001386.1',\n",
       " 'gg6_AADN05001319.1',\n",
       " 'gg6_AADN05001170.1',\n",
       " 'gg6_AADN05001167.1',\n",
       " 'gg6_AADN05001185.1',\n",
       " 'gg6_AADN05001477.1',\n",
       " 'gg6_AADN05001512.1',\n",
       " 'gg6_AADN05001410.1',\n",
       " 'gg6_AADN05001311.1',\n",
       " 'gg6_AADN05001402.1',\n",
       " 'gg6_AADN05001480.1',\n",
       " 'gg6_AADN05001255.1',\n",
       " 'gg6_AADN05001203.1',\n",
       " 'gg6_AADN05001195.1',\n",
       " 'gg6_AADN05001201.1',\n",
       " 'gg6_AADN05001232.1',\n",
       " 'gg6_AADN05001309.1',\n",
       " 'gg6_AADN05001327.1',\n",
       " 'gg6_AADN05001324.1',\n",
       " 'gg6_AADN05001337.1',\n",
       " 'gg6_AADN05001417.1',\n",
       " 'gg6_AADN05001356.1',\n",
       " 'gg6_AADN05001431.1',\n",
       " 'gg6_AADN05001200.1',\n",
       " 'gg6_AADN05001256.1',\n",
       " 'gg6_AADN05001146.1',\n",
       " 'gg6_AADN05001455.1',\n",
       " 'gg6_AADN05001421.1',\n",
       " 'gg6_AADN05001365.1',\n",
       " 'gg6_AADN05001388.1',\n",
       " 'gg6_AADN05001442.1',\n",
       " 'gg6_AADN05001320.1',\n",
       " 'gg6_AADN05001415.1',\n",
       " 'gg6_AADN05001367.1',\n",
       " 'gg6_AADN05001375.1',\n",
       " 'gg6_AADN05001245.1',\n",
       " 'gg6_AADN05001313.1',\n",
       " 'gg6_AADN05001276.1',\n",
       " 'gg6_AADN05001407.1',\n",
       " 'gg6_AADN05001489.1',\n",
       " 'gg6_AADN05001362.1',\n",
       " 'gg6_AADN05001406.1',\n",
       " 'gg6_AADN05001521.1',\n",
       " 'gg6_AADN05001443.1',\n",
       " 'gg6_AADN05001369.1',\n",
       " 'gg6_AADN05001238.1',\n",
       " 'gg6_AADN05001444.1',\n",
       " 'gg6_AADN05001322.1',\n",
       " 'gg6_AADN05001376.1',\n",
       " 'gg6_AADN05001229.1',\n",
       " 'gg6_AADN05001470.1',\n",
       " 'gg6_AADN05001373.1',\n",
       " 'gg6_AADN05001257.1',\n",
       " 'gg6_AADN05001228.1',\n",
       " 'gg6_AADN05001374.1',\n",
       " 'gg6_AADN05001452.1',\n",
       " 'gg6_AADN05001474.1',\n",
       " 'gg6_AADN05001446.1',\n",
       " 'gg6_AADN05001453.1',\n",
       " 'gg6_AADN05001265.1',\n",
       " 'gg6_AADN05001445.1',\n",
       " 'gg6_AADN05001499.1',\n",
       " 'gg6_AADN05001430.1',\n",
       " 'gg6_AADN05001368.1',\n",
       " 'gg6_AADN05001151.1',\n",
       " 'gg6_AADN05001447.1',\n",
       " 'gg6_AADN05001274.1',\n",
       " 'gg6_AADN05001179.1',\n",
       " 'gg6_AADN05001486.1',\n",
       " 'gg6_AADN05001519.1',\n",
       " 'gg6_AADN05001328.1',\n",
       " 'gg6_AADN05001448.1',\n",
       " 'gg6_AADN05001426.1',\n",
       " 'gg6_AADN05001484.1',\n",
       " 'gg6_AADN05001161.1',\n",
       " 'gg6_AADN05001204.1',\n",
       " 'gg6_AADN05001258.1',\n",
       " 'gg6_AADN05001171.1',\n",
       " 'gg6_AADN05001523.1',\n",
       " 'gg6_AADN05001314.1',\n",
       " 'gg6_AADN05001411.1',\n",
       " 'gg6_AADN05001192.1',\n",
       " 'gg6_AADN05001491.1',\n",
       " 'gg6_AADN05001454.1',\n",
       " 'gg6_AADN05001242.1',\n",
       " 'gg6_AADN05001251.1',\n",
       " 'gg6_AADN05001427.1',\n",
       " 'gg6_AADN05001508.1',\n",
       " 'gg6_AADN05001487.1',\n",
       " 'gg6_AADN05001418.1',\n",
       " 'gg6_AADN05001463.1',\n",
       " 'gg6_AADN05001198.1',\n",
       " 'gg6_AADN05001272.1',\n",
       " 'gg6_AADN05001278.1',\n",
       " 'gg6_AADN05001449.1',\n",
       " 'gg6_AADN05001465.1',\n",
       " 'gg6_AADN05001381.1',\n",
       " 'gg6_AADN05001516.1',\n",
       " 'gg6_AADN05001471.1',\n",
       " 'gg6_AADN05001457.1',\n",
       " 'gg6_AADN05001157.1',\n",
       " 'gg6_AADN05001182.1',\n",
       " 'gg6_AADN05001248.1',\n",
       " 'gg6_AADN05001351.1',\n",
       " 'gg6_AADN05001450.1',\n",
       " 'gg6_AADN05001341.1',\n",
       " 'gg6_AADN05001517.1',\n",
       " 'gg6_AADN05001254.1',\n",
       " 'gg6_AADN05001280.1',\n",
       " 'gg6_AADN05001339.1',\n",
       " 'gg6_AADN05001206.1',\n",
       " 'gg6_AADN05001259.1',\n",
       " 'gg6_AADN05001429.1',\n",
       " 'gg6_AADN05001451.1',\n",
       " 'gg6_AADN05001382.1',\n",
       " 'gg6_AADN05001315.1',\n",
       " 'gg6_AADN05001347.1',\n",
       " 'gg6_AADN05001525.1',\n",
       " 'gg6_AADN05000232.1',\n",
       " 'gg6_AADN05001510.1',\n",
       " 'gg6_AADN05001408.1',\n",
       " 'gg6_AADN05001230.1',\n",
       " 'gg6_AADN05001522.1',\n",
       " 'gg6_AADN05001501.1',\n",
       " 'gg6_AADN05001404.1',\n",
       " 'gg6_AADN05001413.1',\n",
       " 'gg6_AADN05001394.1',\n",
       " 'gg6_AADN05001216.1',\n",
       " 'gg6_AADN05001542.1',\n",
       " 'gg6_AADN05001428.1',\n",
       " 'gg6_AADN05001305.1',\n",
       " 'gg6_AADN05001173.1',\n",
       " 'gg6_AADN05001211.1',\n",
       " 'gg6_AADN05001401.1',\n",
       " 'gg6_AADN05001475.1',\n",
       " 'gg6_AADN05001409.1',\n",
       " 'gg6_AADN05001500.1',\n",
       " 'gg6_AADN05001342.1',\n",
       " 'gg6_AADN05001149.1',\n",
       " 'gg6_AADN05001249.1',\n",
       " 'gg6_AADN05001168.1',\n",
       " 'gg6_AADN05001488.1',\n",
       " 'gg6_AADN05001543.1',\n",
       " 'gg6_AADN05001189.1',\n",
       " 'gg6_AADN05001395.1',\n",
       " 'gg6_AADN05001419.1',\n",
       " 'gg6_AADN05001218.1',\n",
       " 'gg6_AADN05001243.1',\n",
       " 'gg6_AADN05001481.1',\n",
       " 'gg6_AADN05001456.1',\n",
       " 'gg6_AADN05001364.1',\n",
       " 'gg6_AADN05001284.1',\n",
       " 'gg6_AADN05001281.1',\n",
       " 'gg6_AADN05001175.1',\n",
       " 'gg6_AADN05001231.1',\n",
       " 'gg6_AADN05001338.1',\n",
       " 'gg6_AADN05001183.1',\n",
       " 'gg6_AADN05001363.1',\n",
       " 'gg6_AADN05001174.1',\n",
       " 'gg6_AADN05001458.1',\n",
       " 'gg6_AADN05001358.1',\n",
       " 'gg6_AADN05001520.1',\n",
       " 'gg6_AADN05001316.1',\n",
       " 'gg6_AADN05001212.1',\n",
       " 'gg6_AADN05001169.1',\n",
       " 'gg6_AADN05001494.1',\n",
       " 'gg6_AADN05001336.1',\n",
       " 'gg6_AADN05001359.1',\n",
       " 'gg6_AADN05001387.1',\n",
       " 'gg6_AADN05001354.1',\n",
       " 'gg6_AADN05001473.1',\n",
       " 'gg6_AADN05001398.1',\n",
       " 'gg6_AADN05001263.1',\n",
       " 'gg6_AADN05001239.1',\n",
       " 'gg6_AADN05001166.1',\n",
       " 'gg6_AADN05001493.1',\n",
       " 'gg6_AADN05001317.1',\n",
       " 'gg6_AADN05001247.1',\n",
       " 'gg6_AADN05001207.1',\n",
       " 'gg6_KZ626825.1',\n",
       " 'gg6_AADN05001479.1',\n",
       " 'gg6_AADN05001391.1',\n",
       " 'gg6_AADN05001492.1',\n",
       " 'gg6_AADN05001262.1',\n",
       " 'gg6_AADN05001260.1',\n",
       " 'gg6_AADN05001301.1',\n",
       " 'gg6_AADN05001156.1',\n",
       " 'gg6_AADN05001209.1',\n",
       " 'gg6_AADN05001277.1',\n",
       " 'gg6_AADN05001184.1',\n",
       " 'gg6_AADN05001396.1',\n",
       " 'gg6_AADN05001155.1',\n",
       " 'gg6_AADN05001296.1',\n",
       " 'gg6_AADN05001384.1',\n",
       " 'gg6_AADN05001294.1',\n",
       " 'gg6_AADN05001180.1',\n",
       " 'gg6_AADN05001321.1',\n",
       " 'gg6_AADN05001556.1',\n",
       " 'gg6_AADN05001194.1',\n",
       " 'gg6_AADN05001304.1',\n",
       " 'gg6_AADN05001208.1',\n",
       " 'gg6_AADN05001511.1',\n",
       " 'gg6_AADN05001268.1',\n",
       " 'gg6_AADN05001397.1',\n",
       " 'gg6_AADN05001236.1',\n",
       " 'gg6_AADN05001176.1',\n",
       " 'gg6_AADN05001298.1',\n",
       " 'gg6_AADN05001380.1',\n",
       " 'gg6_AADN05001164.1',\n",
       " 'gg6_AADN05001379.1',\n",
       " 'gg6_AADN05001252.1',\n",
       " 'gg6_AADN05001285.1',\n",
       " 'gg6_AADN05001360.1',\n",
       " 'gg6_AADN05001186.1',\n",
       " 'gg6_AADN05001266.1',\n",
       " 'gg6_AADN05001269.1',\n",
       " 'gg6_AADN05001261.1',\n",
       " 'gg6_AADN05001544.1',\n",
       " 'gg6_AADN05001392.1',\n",
       " 'gg6_AADN05001325.1',\n",
       " 'gg6_AADN05001563.1',\n",
       " 'gg6_AADN05001509.1',\n",
       " 'gg6_AADN05001150.1',\n",
       " 'gg6_AADN05001502.1',\n",
       " 'gg6_AADN05001310.1',\n",
       " 'gg6_AADN05001550.1',\n",
       " 'gg6_AADN05001318.1',\n",
       " 'gg6_AADN05001485.1',\n",
       " 'gg6_AADN05001403.1',\n",
       " 'gg6_AADN05001560.1',\n",
       " 'gg6_AADN05001215.1',\n",
       " 'gg6_AADN05001162.1',\n",
       " 'gg6_AADN05001178.1',\n",
       " 'gg6_AADN05001554.1',\n",
       " 'gg6_AADN05000623.1',\n",
       " 'gg6_AADN05001196.1',\n",
       " 'gg6_AADN05001333.1',\n",
       " 'gg6_AADN05001190.1',\n",
       " 'gg6_AADN05001270.1',\n",
       " 'gg6_AADN05001273.1',\n",
       " 'gg6_AADN05001154.1',\n",
       " 'gg6_AADN05001163.1',\n",
       " 'gg6_AADN05001279.1',\n",
       " 'gg6_AADN05001297.1',\n",
       " 'gg6_AADN05001295.1',\n",
       " 'gg6_AADN05001559.1',\n",
       " 'gg6_AADN05001540.1',\n",
       " 'gg6_AADN05001148.1',\n",
       " 'gg6_AADN05001383.1',\n",
       " 'gg6_AADN05001177.1',\n",
       " 'gg6_AADN05001352.1',\n",
       " 'gg6_AADN05001514.1',\n",
       " 'gg6_AADN05001299.1',\n",
       " 'gg6_AADN05001344.1',\n",
       " 'gg6_AADN05001507.1',\n",
       " 'gg6_AADN05001152.1',\n",
       " 'gg6_AADN05001495.1',\n",
       " 'gg6_AADN05001515.1',\n",
       " 'gg6_AADN05001472.1',\n",
       " 'gg6_AADN05001334.1',\n",
       " 'gg6_AADN05001153.1',\n",
       " 'gg6_AADN05001332.1',\n",
       " 'gg6_AADN05001300.1',\n",
       " 'gg6_AADN05001244.1',\n",
       " 'gg6_AADN05001253.1',\n",
       " 'gg6_AADN05001548.1',\n",
       " 'gg6_AADN05001246.1',\n",
       " 'gg6_AADN05001303.1',\n",
       " 'gg6_AADN05001539.1',\n",
       " 'gg6_AADN05001213.1',\n",
       " 'gg6_AADN05001490.1',\n",
       " 'gg6_AADN05001555.1',\n",
       " 'gg6_AADN05001340.1',\n",
       " 'gg6_AADN05001286.1',\n",
       " 'gg6_AADN05001282.1',\n",
       " 'gg6_AADN05001552.1',\n",
       " 'gg6_AADN05001557.1',\n",
       " 'gg6_AADN05001541.1',\n",
       " 'gg6_AADN05001158.1',\n",
       " 'gg6_AADN05001561.1',\n",
       " 'gg6_AADN05001335.1',\n",
       " 'gg6_AADN05001181.1',\n",
       " 'gg6_AADN05001147.1',\n",
       " 'gg6_AADN05001210.1',\n",
       " 'gg6_AADN05001159.1',\n",
       " 'gg6_AADN05001558.1',\n",
       " 'gg6_AADN05001346.1',\n",
       " 'gg6_AADN05001551.1',\n",
       " 'gg6_AADN05001553.1',\n",
       " 'gg6_AADN05001545.1',\n",
       " 'gg6_AADN05001564.1',\n",
       " 'gg6_AADN05001546.1',\n",
       " 'gg6_AADN05001547.1',\n",
       " 'gg6_AADN05001549.1',\n",
       " 'gg6_AADN05001562.1']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allchromlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "97a8b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "allchromlist2 = ['hg38_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0bfe1fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca502158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tests/data/test1_train_dataset.bed'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bed_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0389f1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "!cat tests/data/test1_train_dataset.bed | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "884c6d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    },
    {
     "ename": "OutOfBoundsError",
     "evalue": "index 40 is out of bounds for sequence of length 40",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfBoundsError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mGenomeDataIntervalDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallchromlist2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbed_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbed_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfasta_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfasta_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m/lustre/scratch126/gengen/teams/parts/sb79/enformer_loader_sgb/enformer_loader/data.py:28\u001b[0m, in \u001b[0;36mGenomeDataIntervalDataset.__getitem__\u001b[0;34m(self, ind)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ind):\n\u001b[0;32m---> 28\u001b[0m     interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     chr_name, start, end, target \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     30\u001b[0m         interval[\u001b[38;5;241m0\u001b[39m], interval[\u001b[38;5;241m1\u001b[39m], interval[\u001b[38;5;241m2\u001b[39m], interval[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     31\u001b[0m     chr_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchr_bed_to_fasta_map\u001b[38;5;241m.\u001b[39mget(chr_name, chr_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/polars/dataframe/frame.py:10633\u001b[0m, in \u001b[0;36mDataFrame.row\u001b[0;34m(self, index, by_predicate, named)\u001b[0m\n\u001b[1;32m  10630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m  10632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m> 10633\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrow_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  10634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m named:\n\u001b[1;32m  10635\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, row))\n",
      "\u001b[0;31mOutOfBoundsError\u001b[0m: index 40 is out of bounds for sequence of length 40"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for q in GenomeDataIntervalDataset(allchromlist2, bed_file=bed_file, fasta_file=fasta_file):\n",
    "    print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "507a650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = GenomeDataIntervalDataset(allchromlist2, extend_seq=40960, bed_file = bed_file, fasta_file = fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "359cec4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    },
    {
     "ename": "OutOfBoundsError",
     "evalue": "index 40 is out of bounds for sequence of length 40",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfBoundsError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# try to generate a loop for prediction\u001b[39;00m\n\u001b[1;32m      2\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m/lustre/scratch126/gengen/teams/parts/sb79/enformer_loader_sgb/enformer_loader/data.py:28\u001b[0m, in \u001b[0;36mGenomeDataIntervalDataset.__getitem__\u001b[0;34m(self, ind)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ind):\n\u001b[0;32m---> 28\u001b[0m     interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     chr_name, start, end, target \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     30\u001b[0m         interval[\u001b[38;5;241m0\u001b[39m], interval[\u001b[38;5;241m1\u001b[39m], interval[\u001b[38;5;241m2\u001b[39m], interval[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     31\u001b[0m     chr_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchr_bed_to_fasta_map\u001b[38;5;241m.\u001b[39mget(chr_name, chr_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/polars/dataframe/frame.py:10633\u001b[0m, in \u001b[0;36mDataFrame.row\u001b[0;34m(self, index, by_predicate, named)\u001b[0m\n\u001b[1;32m  10630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m  10632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m> 10633\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrow_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  10634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m named:\n\u001b[1;32m  10635\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, row))\n",
      "\u001b[0;31mOutOfBoundsError\u001b[0m: index 40 is out of bounds for sequence of length 40"
     ]
    }
   ],
   "source": [
    "# try to generate a loop for prediction\n",
    "i = 0\n",
    "for sequence, target, loc in test_loader:\n",
    "    print(i)\n",
    "    i += 1\n",
    "    # print(f\"Sequence is {sequence}\") # our test chromosome\n",
    "    # print(f\"target is {target}\") # our test chromosome\n",
    "    # print(f\"loc is {loc}\") # our test chromosome\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc8e24",
   "metadata": {},
   "source": [
    "### Run with imported weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "243f0b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeadAdapterWrapper(\n",
       "  (enformer): Enformer(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (conv_tower): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transformer): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (crop_final): TargetLengthCrop()\n",
       "    (final_pointwise): Sequential(\n",
       "      (0): Rearrange('b n d -> b d n')\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Rearrange('b d n -> b n d')\n",
       "      (3): Dropout(p=0.05, inplace=False)\n",
       "      (4): GELU()\n",
       "    )\n",
       "    (_trunk): Sequential(\n",
       "      (0): Rearrange('b n d -> b d n')\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Rearrange('b d n -> b n d')\n",
       "      (4): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): TargetLengthCrop()\n",
       "      (6): Sequential(\n",
       "        (0): Rearrange('b n d -> b d n')\n",
       "        (1): Sequential(\n",
       "          (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (2): Rearrange('b d n -> b n d')\n",
       "        (3): Dropout(p=0.05, inplace=False)\n",
       "        (4): GELU()\n",
       "      )\n",
       "    )\n",
       "    (_heads): ModuleDict(\n",
       "      (human): Sequential(\n",
       "        (0): Linear(in_features=3072, out_features=5313, bias=True)\n",
       "        (1): Softplus(beta=1.0, threshold=20.0)\n",
       "      )\n",
       "      (mouse): Sequential(\n",
       "        (0): Linear(in_features=3072, out_features=1643, bias=True)\n",
       "        (1): Softplus(beta=1.0, threshold=20.0)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_embed_transform): Sequential(\n",
       "    (0): Identity()\n",
       "  )\n",
       "  (to_tracks): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=1, bias=True)\n",
       "    (1): Softplus(beta=1.0, threshold=20.0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_file = '/lustre/scratch126/gengen/teams/parts/jh47/enformer_pytorch/model_weights/'\n",
    "from enformer_pytorch import from_pretrained\n",
    "from enformer_pytorch.finetune import HeadAdapterWrapper\n",
    "\n",
    "enformer = from_pretrained(weight_file, use_checkpointing = True)\n",
    "model = HeadAdapterWrapper(\n",
    "    enformer = enformer,\n",
    "    num_tracks = 1,\n",
    "    post_transformer_embed = False   # by default, embeddings are taken from after the final pointwise block w/ conv -> gelu - but if you'd like the embeddings right after the transformer block with a learned layernorm, set this to True\n",
    ").cuda()\n",
    "\n",
    "model # we can see the extra layer added at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2ab18951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7574, 0.6448, 0.6450,  ..., 0.6295, 0.7656, 0.6868],\n",
       "        [0.6678, 0.7057, 0.7177,  ..., 0.7218, 0.7363, 0.6881],\n",
       "        [0.7044, 0.6697, 0.6959,  ..., 0.6438, 0.7564, 0.6698],\n",
       "        ...,\n",
       "        [0.6820, 0.7268, 0.6737,  ..., 0.5979, 0.6712, 0.6821],\n",
       "        [0.5947, 0.7043, 0.6316,  ..., 0.7869, 0.7076, 0.6753],\n",
       "        [0.7735, 0.7563, 0.6589,  ..., 0.6367, 0.7537, 0.5957]],\n",
       "       device='cuda:0', grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b96421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
